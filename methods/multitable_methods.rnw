\documentclass{scrartcl}
\usepackage{graphicx, color}
\usepackage{eulervm}
%% maxwidth is the original width if it is less than linewidth
%% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\definecolor{fgcolor}{rgb}{0.2, 0.2, 0.2}
\newcommand{\hlnumber}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlfunctioncall}[1]{\textcolor[rgb]{0.501960784313725,0,0.329411764705882}{\textbf{#1}}}%
\newcommand{\hlstring}[1]{\textcolor[rgb]{0.6,0.6,1}{#1}}%
\newcommand{\hlkeyword}[1]{\textcolor[rgb]{0,0,0}{\textbf{#1}}}%
\newcommand{\hlargument}[1]{\textcolor[rgb]{0.690196078431373,0.250980392156863,0.0196078431372549}{#1}}%
\newcommand{\hlcomment}[1]{\textcolor[rgb]{0.180392156862745,0.6,0.341176470588235}{#1}}%
\newcommand{\hlroxygencomment}[1]{\textcolor[rgb]{0.43921568627451,0.47843137254902,0.701960784313725}{#1}}%
\newcommand{\hlformalargs}[1]{\textcolor[rgb]{0.690196078431373,0.250980392156863,0.0196078431372549}{#1}}%
\newcommand{\hleqformalargs}[1]{\textcolor[rgb]{0.690196078431373,0.250980392156863,0.0196078431372549}{#1}}%
\newcommand{\hlassignement}[1]{\textcolor[rgb]{0,0,0}{\textbf{#1}}}%
\newcommand{\hlpackage}[1]{\textcolor[rgb]{0.588235294117647,0.709803921568627,0.145098039215686}{#1}}%
\newcommand{\hlslot}[1]{\textit{#1}}%
\newcommand{\hlsymbol}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlprompt}[1]{\textcolor[rgb]{0.2,0.2,0.2}{#1}}%

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}

\usepackage{alltt}
% ------------------------------------------------------------------------
% Packages
% ------------------------------------------------------------------------
\usepackage{amsmath,amssymb,amsfonts,graphicx,mathtools,setspace}
\usepackage[body={7in, 9in},left=1in,right=1in]{geometry}
\usepackage{url}
\usepackage[colorlinks]{hyperref}

% ------------------------------------------------------------------------
% Macros
% ------------------------------------------------------------------------
%~~~~~~~~~~~~~~~
% List shorthand
%~~~~~~~~~~~~~~~
\newcommand{\BIT}{\begin{itemize}}
\newcommand{\EIT}{\end{itemize}}
\newcommand{\BNUM}{\begin{enumerate}}
\newcommand{\ENUM}{\end{enumerate}}
%~~~~~~~~~~~~~~~
% Text with quads around it
%~~~~~~~~~~~~~~~
\newcommand{\qtext}[1]{\quad\text{#1}\quad}
%~~~~~~~~~~~~~~~
% Shorthand for math formatting
%~~~~~~~~~~~~~~~
\newcommand\mbb[1]{\mathbb{#1}}
\newcommand\mbf[1]{\mathbf{#1}}
\def\mc#1{\mathcal{#1}}
\def\mrm#1{\mathrm{#1}}
%~~~~~~~~~~~~~~~
% Common sets
%~~~~~~~~~~~~~~~
\def\reals{\mathbb{R}} % Real number symbol
\def\integers{\mathbb{Z}} % Integer symbol
\def\rationals{\mathbb{Q}} % Rational numbers
\def\naturals{\mathbb{N}} % Natural numbers
\def\complex{\mathbb{C}} % Complex numbers
%~~~~~~~~~~~~~~~
% Common functions
%~~~~~~~~~~~~~~~
\renewcommand{\exp}[1]{\operatorname{exp}\left(#1\right)} % Exponential
\def\indic#1{\mbb{I}\left({#1}\right)} % Indicator function
\providecommand{\argmax}{\mathop\mathrm{arg max}} % Defining math symbols
\providecommand{\argmin}{\mathop\mathrm{arg min}}
\providecommand{\maximize}{\mathop\mathrm{maximize\quad}} % Defining math symbols
\providecommand{\minimize}{\mathop\mathrm{minimize\quad}} % Defining math symbols
\providecommand{\arccos}{\mathop\mathrm{arccos}}
\providecommand{\dom}{\mathop\mathrm{dom}} % Domain
\providecommand{\range}{\mathop\mathrm{range}} % Range
\providecommand{\diag}{\mathop\mathrm{diag}}
\providecommand{\tr}{\mathop\mathrm{tr}}
\providecommand{\abs}{\mathop\mathrm{abs}}
\providecommand{\card}{\mathop\mathrm{card}}
\providecommand{\sign}{\mathop\mathrm{sign}}
\def\rank#1{\mathrm{rank}({#1})}
\def\supp#1{\mathrm{supp}({#1})}
%~~~~~~~~~~~~~~~
% Common probability symbols
%~~~~~~~~~~~~~~~
\def\E{\mathbb{E}} % Expectation symbol
\def\Earg#1{\E\left[{#1}\right]}
\def\Esubarg#1#2{\E_{#1}\left[{#2}\right]}
\def\P{\mathbb{P}} % Probability symbol
\def\Parg#1{\P\left({#1}\right)}
\def\Psubarg#1#2{\P_{#1}\left[{#2}\right]}
\def\Corr{\mrm{Corr}} % Correlation symbol
\def\Cov{\mrm{Cov}} % Covariance symbol
\def\Covarg#1{\Cov\left[{#1}\right]}
\def\Covsubarg#1#2{\Cov_{#1}\left[{#2}\right]}
\def\Corrsubarg#1#2{\Corr_{#1}\left[{#2}\right]}
\def\Var{\mrm{Var}}
\def\Vararg#1{\Var\left(#1\right)}
\def\Varsubarg#1#2{\Var_{#1}\left(#2\right)}
\newcommand{\family}{\mathcal{P}} % probability family
\newcommand{\eps}{\epsilon}
\def\absarg#1{\left|#1\right|}
\def\msarg#1{\left(#1\right)^{2}}
\def\logarg#1{\log\left(#1\right)}
%~~~~~~~~~~~~~~~
% Distributions
%~~~~~~~~~~~~~~~
\def\Gsn{\mathcal{N}}
\def\Ber{\textnormal{Ber}}
\def\Bin{\textnormal{Bin}}
\def\Unif{\textnormal{Unif}}
\def\Mult{\textnormal{Mult}}
\def\NegMult{\textnormal{NegMult}}
\def\Dir{\textnormal{Dir}}
\def\Bet{\textnormal{Beta}}
\def\Poi{\textnormal{Poi}}
\def\HypGeo{\textnormal{HypGeo}}
\def\GEM{\textnormal{GEM}}
\def\BP{\textnormal{BP}}
\def\DP{\textnormal{DP}}
\def\BeP{\textnormal{BeP}}
%~~~~~~~~~~~~~~~
% Theorem-like environments
%~~~~~~~~~~~~~~~
\newtheorem{definition}{Definition}
\newtheorem{example}{Example}
\newtheorem{theorem}{Theorem}

%-----------------------
% Probability sets
%-----------------------
\newcommand{\X}{\mathcal{X}}
\newcommand{\D}{\mathcal{D}}
\newcommand{\Y}{\mathcal{Y}}
%-----------------------
% vector notation
%-----------------------
\newcommand{\bx}{\mathbf{x}}
\newcommand{\by}{\mathbf{y}}
\newcommand{\bt}{\mathbf{t}}
\newcommand{\xbar}{\overline{x}}
\newcommand{\Xbar}{\overline{X}}
\newcommand{\tolaw}{\xrightarrow{\mathcal{L}}}
\newcommand{\toprob}{\xrightarrow{\mathbb{P}}}
\newcommand{\laweq}{\overset{\mathcal{L}}{=}}
\newcommand{\F}{\mathcal{F}}
\renewenvironment{knitrout}{\begin{singlespace}}{\end{singlespace}}
%-----------------------
% Math environments
%-----------------------
%\newcommand{\ba}{\begin{align}}
%\newcommand{\ea}{\end{align}}
%\newcommand{\ba}{\begin{align}}

<<echo = FALSE>>=
library("knitr")
library("ggplot2")
library("grid")
opts_chunk$set(cache = TRUE, fig.width = 4, fig.height = 4, fig.align = "center")

scale_colour_discrete <- function(...)
  scale_colour_brewer(..., palette="Set2")
small_theme <- function(base_size = 7, base_family = "Helvetica") {
  theme_bw(base_size = base_size, base_family = base_family) %+replace%
  theme(axis.title = element_text(size = rel(0.8)),
        legend.title = element_text(size = rel(0.8)),
        legend.text = element_text(size = rel(0.8)),
        plot.title = element_text(size = rel(1.1)),
        plot.margin = unit(c(0.1, .1, .1, .1), "lines")
        )
}

#read_chunk()
@

\linespread{1.5}

\begin{document}

\section{Classical multivariate methods}

Methods from classical multivariate statistics have become a mainstay
of single-table microbiome data analysis, so it is natural to revisit
this literature to identify extensions to the multitable
setting. Here we describe a few of the classically studied multitable
methods that fit nicely into the modern microbiome data analysis
toolbox. We first describe a naive approach based on Principal
Components Analysis (PCA) -- naive because it lifts a single-table
method to the multiple table setting without any special
considerations --  before studying approaches that directly
characterize covariation across several tables: Canonical Correlation
Analysis (CCA), Multiple Factor Analysis (MFA), and Principal
Component Analysis with Instrumental Variables (PCA-IV).

But first, a comment on the setting in which these methods
emerged. The earliest multitable method (CCA) was published in 1936,
where the motivating data analysis problem was to relate prices of
groups of commodities \cite{hotelling1936relations}. There are two notable
aspects of data analysis in this classical paradigm which no longer
hold in modern statistics:
\begin{itemize}
  \item Even when many samples could be collected, there were
    typically only a few features for each sample, and it was
    straightforwards to study all of them simultaneously. In the last
    few decades, it has become possible to automatically collect a
    large number of features for each sample.
  \item Before electronic computers had been invented, it was
    important that all statistical quantities be relatively easy to
    calculate. This is no longer a restriction in an environment with
    rich computational resources.
\end{itemize}

These changes have motivated the need for high-dimensional methods and
facilitated the adoption of iterative, more computationally-intensive
approaches, respectively, some of which are described in the
description below.

Nonetheless, it is important to review these original approaches, both to
understand the context for many modern techniques, as well as to have
an easy starting point for practical data analysis. Indeed, these more
established, tend to be the most readily available through statistical
computing packages and can often provide a benchmark with which to
compare more elaborate, modern methods.

\subsection{PCA}
\label{sec:pca}

The simplest approach to dealing with multiple tables is to combine
them into one and apply a single-table method, for example, PCA. That
is, write
\begin{align}
X = \left[X^{(1)} \vert \dots \vert X^{(L)}\right] \in \reals^{n \times p},
\end{align}
where $p = \sum_{l = 1}^{L}p_{l}$, and compute the SVD\footnote{An
  equivalent procedure is to eigenanalyze the empirical covariance
  matrix $\frac{1}{n}X^{T}X$.}, $X = UDV^{T}$. The $k$-principal
component directions as the first $k$ columns $V\left[, 1:k\right]$,
while the associated scores $\left(UD\right)\left[, 1:k\right]$.

While this does not account for the multitable structure of the data,
it does accomplish two goals,
\begin{enumerate}
\item Through the principal component scores, it provides a
  visualization of the relationships between
  samples, based on all features.
\item Through the principal component directions, it gives a way of
  relating features within and across the multiple tables.
\end{enumerate}

However, two important drawbacks of this approach are
\begin{enumerate}
  \item It does not provide a summary of the relationship between the
    sets of variables defining the tables -- it can only relate pairs
    of variables. \label{bullet:pca_drawback_one}
  \item If some tables have many more variable than others, they can
    dominate the resulting ordination. \label{bullet:pca_drawback_two}
\end{enumerate}

These limitations are addressed by CCA and MFA.

There are many ways to motivate the procedure behind PCA, two standard
ones come from a geometric and a statistical point of view. The
geometric motivation is that, if each row $x_{i}$ of $X$ is viewed as
a point in $p$-dimensional space, then the principal component
directions provide the best $k$-dimensional approximation to the
data. Formally, recall that $VV^{T}x_{i}$ is the projection of $x_{i}$
onto the subspace spanned by the columns of $V$. PCA identifies the
orthogonal matrix $V$ such that
\begin{align}
\sum_{i = 1}^{n}\|x_{i} - VV^{T} x_{i}\|_{2}^{2}
\end{align}
is minimized. The principal component scores are then the coordinate
of the projected points with respect to this subspace.

The second interpretation is that PCA finds a low-dimensional
representation of the $x_{i}$ such that the resulting points have
maximal variance. Speaking qualitatively, this is a desirable
property, because it means that the (simpler) representation
``preserves most of the variation'' present in the original
data. Formally, suppose that the $x_{i}\in\reals^{p}$ are drawn
independently from some distribution $\P$, so that the variance is
$\Covsubarg{\P}{x_{i}} = \Sigma$ . Consider an arbitrary
linear combination of $x_{i}$'s $p$ coordinates: $z_{i} := c^{T}x_{i}$
for some $c \in \reals^{p}$. The first PCA direction gives the $c$
such that the variance of this coordinate, $\Covsubarg{\P}{z_{i}} =
c^{T}\Sigma c$, is maximal. The second direction gives the linear
combination that maximizes variance, subject to being orthogonal to
the first, and so forth.

While our description of the method of concatenating multiple tables
into a single one has focused on PCA, we note as an aside that other
methods could be applied. For example, it is possible to define a new
distance between samples as a mixture of distances based on several
tables. This can be useful if there are different types of data across
the different tables: Jaccard, chi-squared, and euclidean distsances
can be applied to binary, count, and real valued tables, for
example. The combined distance can then be input into any
distance-based procedure, like multidimensional scaling (MDS) or
hierarchical clustering. The primary downside of this approach is that
the resulting distance only allows a comparison across samples, but
not between features.

PCA is a very widely used technique, and some standard references
include \cite{friedman2001elements, mardia1980multivariate,
  pages2014multiple}.

\subsection{CCA}

Canonical Correlation Analysis is a close relative of PCA that
explicitly compares sets of features across multiple tables. While it
continues to provide low-dimensional representations of samples, as in
PCA, it remedies the problem \ref{bullet:pca_drawback_one} associated
with the naive approach of performing PCA on concatenated data.

Suppose for now that there are only two tables, $X \in \reals^{n
  \times p_{1}}$ and $Y \in \reals^{n \times p_{2}}$,
upon which we want to base an ordination.

Let $\hat{\Sigma}_{XX}, \hat{\Sigma}_{YY}$, and $\hat{\Sigma}_{XY}$
be the associated covariance estimates. Take the SVD,
$\Sigma_{XX}^{-\frac{1}{2}}\Sigma_{XY}\Sigma_{YY}^{-\frac{1}{2}} =
\tilde{U}D\tilde{V}^{T}$. The canonical correlation directions
associated with the two tables are $u_{k} =
\Sigma_{XX}^{-\frac{1}{2}}\tilde{u}_{k} \in \reals^{p_{1}}$ and
$v_{k}^{(2)} = \Sigma_{YY}^{-\frac{1}{2}}\tilde{v}_{k} \in
\reals^{p_{2}}$. These directions give two low-dimensional
representations for each sample, one for each table: $z_{k}^{(1)} =
Xu_{k} \in \reals^{n}$ and $z_{k}^{(2)} = Yv_{k} \in \reals^{n}$. If
the two tables are very closely related, then the $z_{k}^{(1)}$ and
$z_{k}^{(2)}$ should be very correlated. The singular values $d_{k}$
are called the canonical correlation coefficients. Like the
eigenvalues in PCA, they characterize the amount of covariation across
tables that can be captured by the first $k$ recovered directions.

As in PCA, there are geometric, statistical, and probabilistic
interpretations for this procedure. Unlike the geometric
interpretation for PCA, the geometric interpretation for CCA identifies
each feature with a point, not each sample. Specifically, the columns
of $X$ and $Y$ with points in $\reals^{n}$. Consider the two subspaces
defined as the spans of the columns of $X$ and $Y$,
respectively. These subspaces correspond to the linear combinations of
features within each table. Consider the two ellipses that lie on the
respective subspaces, whose size and shape depends on the within table
covariances $\Sigma_{XX}$ and $\Sigma_{YY}$. The first canonical
correlation directions are the pair of points, one lying on each
ellipse, such that the angle from the origin to those two points is
smallest. In this sense, it finds a pair of linear combinations of
features within the two tables so that the two tables appear ``close''
to one another. The second pair of canonical correlation directions
identify a pair of points with similar interpretation, except they are
required to be orthogonal to the first pair, with respect to the inner
product induced by the covariances for each table.

For the statistical interpretation, the idea of CCA is to find the
low-dimensional representations of the two tables with maximal
covariance; this is analogous to PCA, which finds the low-dimensional
representation of one table with maximal variance. Formally, let
$x_{i}$ and $y_{i}$ samples from $\P^{\mathcal{X}}$ and
$\P^{\mathcal{Y}}$. The rows of the two tables are imagined to be iid
draws from $\P^{\X\Y}$. Consider arbitrary linear combinations
$z_{i}^{(1)}\left(u\right) = u^{T} x_{i}$ and
$z_{i}^{(2)}\left(v\right) = v^{T}y_{i}$ of the columns in the two
tables. The first pair of CCA directions $u_{1}^{\ast}$ and
$v_{1}^{\ast}$ are chosen to optimize
\begin{align}
  \maximize_{u \in \reals^{p_{1}}, v \in \reals^{p_{2}}}
  &\Covsubarg{\P^{\X\Y}}{z_{i}^{(1)}\left(u\right),
    z_{i}^{(2)}\left(v\right)} \label{eq:cancor_optim} \\
\text{subject to } &\Varsubarg{\P^{\X}}{z_{i}^{(1)}\left(u\right)} = 1 \\
&\Varsubarg{\P^{\Y}}{z_{i}^{(2)}\left(v\right)} = 1,
\end{align}
where $\P^{\X}$ and $\P^{\Y}$ denote the marginals associated with
$\P^{\X\Y}$. To produce subsequent directions, the same optimization
is performed, but with the additional constraint that the directions
must be orthogonal to all the previous directions identified for that
table. Of course, in actual applications, we estimate these
covariances and variances empirically.

This perspective makes it easy to derive the algorithm to produce the
CCA directions stated above. The empirical version of the optimization
problem \ref{eq:cancor_optim} is
\begin{align}
  \maximize_{u \in \reals^{p_{1}}, v \in \reals^{p_{2}}}
  &u^{T}\hat{\Sigma}_{XY}v \label{eq:cancor_optim_emp}\\
  \text{subject to } & u^{T}\hat{\Sigma}_{XX}u = 1 \\
  & v^{T}\hat{\Sigma}_{YY}v = 1.
\end{align}

Consider the whitened version of these variables, where $\tilde{u} =
\hat{\Sigma}_{XX}^{\frac{1}{2}}u$ and $\tilde{v} =
\hat{\Sigma}_{YY}^{\frac{1}{2}}v$. The
optimization \label{eq:cancor_emp} can now be expressed as
\begin{align}
  \maximize_{\tilde{u} \in \reals^{p_{1}}, \tilde{v} \in
    \reals^{p_{2}}}
  \tilde{u}^{T}\hat{\Sigma}_{XX}^{-\frac{1}{2}}\hat{\Sigma}_{XY}\hat{\Sigma}_{YY}^{-\frac{1}{2}}\tilde{v} \label{eq:cancor_trans}
  \\
\text{such that } & \|\tilde{u}\|^{2}_{2} = 1 \\
&= \|\tilde{v}\|_{2}^{2} = 1.
\end{align}
The optimal $\tilde{u}^{\ast}$ and $\tilde{v}^{\ast}$ for this problem
are well known -- they're exactly the first left and right
eigenvectors of
$\hat{\Sigma}_{XX}^{-\frac{1}{2}}\hat{\Sigma}_{XY}\hat{\Sigma}_{YY}^{-\frac{1}{2}}
= \tilde{U}D\tilde{V}^{T}$, respectively. The argument is standard,
but we include it for completeness.

To this end, consider arbitrary potential maximizers $\xi$ and $\nu$
of length one. We can find $w_{u}$ and $w_{v}$ such that $\xi =
\tilde{U}w_{u}, \nu = \tilde{V}w_{v}$, since $\tilde{U}$ and
$\tilde{V}$ are both orthonormal bases. Further, since they are length
one, $1 = \|\xi\|^{2}_{2} = w_{u}^{T}\tilde{U}^{T}\tilde{U}w_{u} =
\|w_{u}\|_{2}^{2}$ and similarly $\|w_{v}\|_{2}^{2} = 1$. The
objective \ref{eq:cancor_trans} can be bounded by
\begin{align}
\xi^{T}\hat{\Sigma}_{XX}^{-\frac{1}{2}}\hat{\Sigma}_{XY}\hat{\Sigma}_{YY}^{-\frac{1}{2}}\nu
&= w_{u}^{T}\tilde{U}^{T}\tilde{U}D\tilde{V}^{T}\tilde{V}w_{v} \\
&=  w_{u}^{T}Dw_{v} \\
&= \sum_{k = 1}^{p_{1} \wedge p_{2}} d_{k}w_{uk}w_{vk} \\
&\leq d_{1} \sum_{k = 1}^{p_{1} \wedge p_{2}} w_{uk}w_{vk} \\
&\leq d_{1} \|w_{u}\|\|w_{v}\| = d_{1},
\end{align}
and this maximum is attained when $w_{u}$ and $w_{v}$ both put all
their weight on the first coordinate, that is $\xi = \tilde{u}_{1}$
and $\nu = \tilde{v}_{1}$. For subsequent directions, we repeat the
argument but require that $w_{u}$ and $w_{v}$ have zero weight on the
first columns of $\tilde{U}$ and $\tilde{V}$.

A probabilistic interpretation of this procedure views it as
estimating the factors in an implicit latent variable model. In
particular, \cite{bach2005probabilistic} supposed that $x_{i}$
and $y_{i}$ were drawn iid from the following model,
\begin{align}
  \xi_{i} &\sim \Gsn\left(0, I_{d}\right) \\
  x_{i} \vert \xi_{i} &\sim \Gsn\left(W_{X}\xi_{i} + \mu_{X},
    \Psi_{1}\right) \\
  y_{i} \vert \xi_{i} &\sim \Gsn\left(W_{Y}\xi_{i} + \mu_{Y},
    \Psi_{Y}\right).
\end{align}
That is, each sample is associated with a $k$-dimensional latent
variable $\xi_{i}$, drawn from a spherical normal prior. The two tables
give features $y_{i}$ and $x_{i}$ associated with this
sample, which are thought to be different noisy linear combinations of
this latent variable. The authors demonstrated that the posterior
expectations of the latent $\xi_{i}$ given the observed tables must lie
on the subspace defined by the CCA directions. More precisely,
\begin{align}
  \Earg{\xi_{i} \mid x_{i}^{(1)}} \in \text{span}\left(z_{1}^{(1)},
    \dots, z_{k}^{(1)}\right),
\end{align}
and
\begin{align}
  \Earg{\xi_{i} \mid x_{i}^{(2)}} \in \text{span}\left(z_{1}^{(2)},
    \dots, z_{k}^{(2)}\right).
\end{align}

(todo: I'm not so sure about this anymore, see page 409 of Murphy)

Finally, we observe that the logic for CCA generalizes to an arbitrary
number $L$ of tables, by summing all pairwise covariances. That is,
the instead of finding directions $c_{k}^{(1)}$ and $c_{k}^{(2)}$
maximizing $\Covsubarg{\P^{(1)}, \P^{(2)}}{c_{k}^{(1)T}x_{i}^{(1)},
  c_{k}^{(2)T}x_{i}^{(2)}}$ subject to normalization and orthogonality
constraints, we seek directions $c_{k}^{(1)}, \dots, c_{k}^{(L)}$ that
maximize the sum of cross-covariances $\sum_{l,l^{\prime} = 1}^{L}
\Covsubarg{\P^{(l)}, \P^{(l^{\prime})}}{c_{k}^{(l) T}x_{i}^{(l)},
  c_{k}^{(l^{\prime})}x_{i}^{(l^{\prime})}}$.

\subsection{Co-Inertia Analysis}

Co-Inertia Analysis (CoIA) was motivated by the same problem as
Canonical Correspondence Analysis, where the main question is how
groups of species become more or less abundant depending on the
environmental conditions at the sites they were observed
\cite{doledec1994co}. It can be viewed as a slight
modification\footnote{I might even call it a simplification.} of
Canonical Correlation Analysis. Again, we seek sets of orthonormal
directions $\left(u_{k}\right)_{k = 1}^{K}$ and $\left(v_{k}\right)_{k
  = 1}^{K}$ such that the associated projections $X^{(1)}u_{k}$ and
$X^{(2)}v_{k}$ explain most of the covariation between the
tables. Unlike Canonical Correlation Analysis, the approach is to
maximize the covariance between the scores, not the
correlation. Formally, it chooses the first directions $u_{1}$ and
$v_{1}$ as the solutions to
\begin{align}
\maximize_{u \in \reals^{p_{1}}, v \in \reals^{p_{2}}} &u^{T}X^{T}Yv \\
\text{ such that}\medspace &\|u\| = \|v\| = 1,
\end{align}
and subsequent directions by the same optimization, with the
additional constraint that they are orthogonal to the previously
derived directions, $u_{k^{\prime}} \perp
\text{span}\left(\left(u_{k}\right)_{k = 1}^{k^{\prime} - 1}\right)$
and $v_{k^{\prime}} \perp \text{span}\left(\left(v_{k}\right)_{k =
    1}^{k^{\prime} - 1}\right)$.

Comparing with the CCA formulation \ref{eq:cancor_optim}, we see that
the only difference is that the norm constraint is imposed on $u$ and
$v$ directly, rather than their projections. Indeed, forcing their
projections to be length one makes the CCA objective the correlation
between scores, while in CoIA it is only the covariance.

The solutions $\left(u_{k}\right)_{k = 1}^{K}$ and
$\left(v_{k}\right)_{k = 1}^{K}$ can be obtained as the first $K$ left
and right eigenvectors from the SVD of $X^{T}Y$, as opposed to the
first $K$ generalized eigenvectors, as in CCA. By essentially the
same argument as in CCA, but we include it for completeness.
\subsection{MFA}

Multiple factor analysis gives an alternative approach to producing
scores and relating features across multiple
tables\cite{pages2014multiple}. It can be understood as a reweighted
version of the concatenated PCA described in section \ref{sec:pca}
that reweights tables in a way that avoids issues
\ref{bullet:pca_drawback_one} and
\ref{bullet:pca_drawback_two}. Specifically, perform PCA on the matrix
\begin{align}
X := \left[\frac{1}{\lambda_{1}\left(X^{(1)}\right)}X^{(1)} \vert \dots
  \vert \frac{1}{\lambda_{1}\left(X^{(L)}\right)}X^{(L)}\right],
\end{align}
which reweights each table by its largest eigenvalue. This procedure
is the multitable analog of the standard practice of standardizing
variables before performing PCA, and it resolves issue
\ref{bullet:pca_drawback_two}.

The resulting MFA directions and scores can be interpreted in the same
way as those from PCA. That is, the MFA directions still specify the
relationship between measured features, and the position of each
sample's projection describes the relative weight of each feature for
that sample. Moreover, MFA gives a way of comparing full tables to
each other, called a ``canonical analysis'' in the MFA literature
\cite{pages2004multiple}. A $K$-dimensional representation of the
$l^{th}$ group is given by
\begin{align}
\left[\mathcal{L}\left(z_{1}, X^{(l)}\right) \dots,
  \mathcal{L}\left(z_{K}, X^{(l)}\right)\right]
\end{align}
where $z_{k} = d_{k}u_{k} \in \reals^{n}$ are the $k^{th}$ column of
principal component scores and
\begin{align}
  \mathcal{L}\left(z_{k}, X^{(l)}\right) =
  \frac{\lambda_{k}\left(X\right)}{\lambda_{1}\left(X^{(l)}\right)}\tr\left(X^{(l)}X^{(l)
      T} z_{k}z_{k}^{T}\right) =
  \frac{\lambda_{k}\left(X\right)}{\lambda_{1}\left(X^{(l)}\right)}\|X^{(l)
  T} u_{k}\|^{2}_{2}
\end{align}
is a measure of aggregate similarity between the coordinates in the
$l^{th}$ table and the $k^{th}$ column of scores. In this definition,
if the samples, as represented by the $l^{th}$ table, have high
correlation with the $k^{th}$ dimension of scores, then the summary
for that group will be projected far in the $k^{th}$ direction.

\subsection{PCA-IV}

Principal components with instrumental variables (PCA-IV)
\cite{rao1964use} adapts the dimension reduction ideas of PCA to
multivariate regression setting. It can also be viewed as a
version of PCA that chooses a dimension reduction of $X^{(2)}$ in
light of its ability to predict $X^{(1)}$, a kind of supervised
version of CCA. In this sense, it anticipates methods like partial
least squares, canonical correspondence analysis, the curd \& whey
procedure, and various approaches to multitask learning, see the
discussion below.

For the formal setup, suppose we are predicting $x_{i}^{(1)}
\in \reals^{p_{1}}$ from $x_{i}^{(2)} \in \reals^{p_{2}}$. Since
$p_{2}$ may be large, it might be useful to work with a
lower-dimensional representation $z_{i} = V^{T}x^{(2)}_{i} \in
\reals^{K}$, that is potentially more interpretable and is still
predictive of $x^{(1)}$. As in PCA, we want the columns of $V$ to be
orthonormal.

The criterion that PCA-IV uses to identify the weights $V$ and
scores $Z$ is similar to the maximum variance view of PCA. Instead of
choosing $V$ to maximize the variance of the $z_{i}$, we choose it to
minimize the residual covariance of $x_{i}^{(1)}$ given $z_{i}$. That
is, suppose that $x_{i}^{(1)}$ and $x_{i}^{(2)}$ are normal with mean
0 (we can always center the data) and joint covariance
\begin{align}
\Var_{\P}\begin{pmatrix}x_{i}^{(1)} \\ x_{i}^{(2)}\end{pmatrix} &=
\begin{pmatrix}
  \Sigma_{11} & \Sigma_{12} \\
  \Sigma_{21} & \Sigma_{22}
\end{pmatrix}.
\end{align}

If $z_{i} = V^{T}x_{i}^{(2)}$, then the joint covariance of
$x_{i}^{(1)}$ and $z_{i}$ is
\begin{align}
  \Var_{\P}\begin{pmatrix} x_{i}^{(1)} \\ z_{i} \end{pmatrix} &=
  \begin{pmatrix}
    \Sigma_{11} & \Sigma_{12}V \\
    V^{T}\Sigma_{21} & V^{T}\Sigma_{22}V
  \end{pmatrix},
\end{align}
and the residual covariance of $x_{i}^{(1)}$ given $z_{i}$ is
\begin{align}
  \Sigma_{11} -
  \Sigma_{12}V\left(V^{T}\Sigma_{22}V\right)^{-1}V^{T}\Sigma_{21}. \label{eq:pca_iv_resid_cov}
\end{align}
There are several ways to measure the size of this matrix, but
\cite{rao1964use} uses the trace. The true population covariances are
unknown to us, so we replace them by their empirical estimates. The
formal optimization for PCA-IV then becomes
\begin{align}
  \minimize_{V\in \reals^{p_{2} \times K} \text{ orthonormal}}
  \tr\left(\hat{\Sigma}_{11} -
    \hat{\Sigma}_{12}V\left(V^{T}\hat{\Sigma}_{22}V\right)^{-1}V^{T}\hat{\Sigma}_{21}\right). \label{eq:pca_iv_obj},
\end{align}
or, equivalently,
\begin{align}
  \maximize_{V\in \reals^{p_{2} \times K} \text{ orthonormal}}
  \tr\left(\hat{\Sigma}_{12}V\left(V^{T}\hat{\Sigma}_{22}V\right)^{-1}V^{T}\hat{\Sigma}_{21}\right). \label{eq:pca_iv_obj_2},
\end{align}

The optimal $V$ are the top $k$ generalized eigenvectors of
$\hat{\Sigma}_{21}\hat{\Sigma}_{12}$ with respect to $\hat{\Sigma}_{22}$,
that is, the orthonormal set of $\left(v_{k}\right)$ satisfying
\begin{align}
\hat{\Sigma}_{21}\hat{\Sigma}_{12}v_{k} &= \lambda_{k}
\hat{\Sigma}_{22}v_{k},
\end{align}
which can be written more concisely as
\begin{align}
\hat{\Sigma}_{21}\hat{\Sigma}_{12}V &= \left( \lambda_{1}
  \hat{\Sigma}_{22}v_{1} \vert \dots \vert
  \lambda_{k}\hat{\Sigma}_{22}v_{k}\right) =
\hat{\Sigma}_{22}V\Lambda,
\end{align}
where $\Lambda = \diag\left(\lambda_{k}\right) \in \reals^{K \times K}$. In
particular, $\hat{\Sigma}_{21}\hat{\Sigma}_{12}$ has generalized
eigendecomposition $\hat{\Sigma}_{21}\hat{\Sigma}_{12} = \hat{\Sigma}_{22} V\Lambda V^{T}$.

To see why this is optimal, first consider $k = 1$. Then, for any
$\tilde{v}$, the objective \ref{eq:pca_iv_obj_2} has the form
\begin{align}
  \tr\left(\hat{\Sigma}_{12}\tilde{v}\left(\tilde{v}\hat{\Sigma}_{22}\tilde{v}\right)^{-1}
    \left(\hat{\Sigma}_{12}\tilde{v}\right)^{T}\right) &=
  \frac{\tilde{v}^{T}\Sigma_{21}\Sigma_{12}\tilde{v}}{\tilde{v}^{T}\Sigma_{22}\tilde{v}} \label{eq:gev_opt_1}\\
  &=
  \frac{\tilde{w}^{T}\Sigma_{22}^{-\frac{1}{2}}\Sigma_{21}\Sigma_{12}\Sigma_{22}^{-\frac{1}{2}}\tilde{w}}{\|\tilde{w}\|_{2}^{2}}, \label{eq:gev_opt_2}
\end{align}
where we change variables $\tilde{w} =
\Sigma_{22}^{\frac{1}{2}}\tilde{v}$. But to maximize
  \ref{eq:gev_opt_2}, just choose $\tilde{w}$ to be the top
  eigenvector of
  $\Sigma_{21}^{-\frac{1}{2}}\Sigma_{21}\Sigma_{12}\Sigma_{21}^{-\frac{1}{2}}$,
which implies that $\tilde{v}$ is the top generalized eigenvector of
$\Sigma_{21}\Sigma_{12}$ with respect to $\Sigma_{22}$. Indeed, in
this case,
\begin{align}
  \Sigma_{21}\Sigma_{12}\tilde{v}
  =\Sigma_{21}\Sigma_{12}\Sigma_{22}^{-\frac{1}{2}}\tilde{w} =
  \Sigma_{22}^{\frac{1}{2}} \Sigma_{22}^{-\frac{1}{2}}\Sigma_{21}\Sigma_{12} \Sigma_{22}^{\frac{1}{2}}\tilde{w}
  = \Sigma_{22}^{\frac{1}{2}}\lambda_{1}\tilde{w}
  = \lambda_{1}\Sigma_{22}\tilde{v}.
\end{align}

So indeed, the criterion is maximized by the top generalized
eigenvector, in the case that $K = 1$. For larger $K$, recall that the
problem of maximizing $\frac{v^{T}Av}{\|v\|^{2}}$ over $v$ subject to
being orthogonal to the first $k - 1$ eigenvectors of $A$ is solved by
the $k^{th}$ eigenvector of $A$, so applying this fact in step
\ref{eq:gev_opt_2} of the argument above gives the result for general
$K$.

For a geometric interpretation of PCA-IV\footnote{I haven't read
  anything about this so (1) I'm a little proud I came up with
  something and (2) everything here might be wrong.}, consider each
column $x_{\cdot j}^{(1)}$ in $X^{(1)}$ and $x_{\cdot j}^{(2)}$ in
$X^{(2)}$ as a point in $\reals^{n}$. The $\left(x_{\cdot
    j}^{(2)}\right)_{j = 1}^{p_{2}}$ span some $p^{(2)}$-dimensional
subspace (we assume full rank). The $\left(x_{\cdot j}^{(1)}\right)$
are $p^{(1)}$ points in this same space. A set of independent
regressions of columns of $X^{(2)}$ onto $X^{(1)}$ simply projects the
$x_{\cdot j}^{(1)}$ onto the span of $\left(x_{\cdot j}^{(2)}\right)$,
and the residuals are the distance to this span. The PCA-IV procedure
is an attempt to find a further $K$-dimensional subspace within the
span of the $\left(x_{\cdot j}^{(2)}\right)$ such that the residuals
of the regressions from $x_{\cdot j}^{(1)}$ onto this further subspace
is not much worse.

Indeed, write the usual estimates for the covariance matrices of
interest,
\begin{align}
  \Sigma_{11} = \frac{1}{n}X^{(1) T}X^{(1)} \\
  \Sigma_{12} = \frac{1}{n}X^{(1) T}X^{(2)} \\
  \Sigma_{22} = \frac{1}{n}X^{(2) T}X^{(2)},
\end{align}
and observe that the residual covariance \ref{eq:pca_iv_resid_cov} can
be expressed
\begin{align}
  &\frac{1}{n}\left[X^{(1) T}X^{(1)} - X^{(1)
      T}X^{(2)}V\left(V^{T}X^{(2) T}X^{(2)}V\right)^{-1}V^{T}X^{(2)
      T}X^{(1)}\right] \\
  = &\frac{1}{n}\left[X^{(1) T}X^{(T)} - X^{(1)
      T}Z\left(Z^{T}Z\right)^{-1}Z^{T} X^{(1)}\right] \\
  = &X^{(1)}\left(I - P_{Z}\right)X^{(1)},
\end{align}
where $P_{Z} = Z\left(Z^{T}Z\right)^{-1}Z^{T}$ is the projection
operator onto the columns of $Z$. Now, minimizing the trace of this
matrix is equivalent to minimizing
\begin{align}
  \tr\left(X^{(1) T}\left(I - P_{Z}\right)X^{(1)}\right) &=
  \tr\left(X^{(1) T}\left(I - P_{Z}\right)^{T}\left(I -
      P_{Z}\right)X^{(1)}\right) \\
  &= \|\left(I - P_{Z}\right)X^{(1)}\|_{F}^{2} \\
  &= \sum_{j = 1}^{p_{1}}\|\left(I - P_{j}\right)x_{\cdot j}^{(1)}\|^{2}_{2},
\end{align}
which is exactly the sum of squared residuals from the columns of
$X^{(1)}$ onto the span of the PCA-IV subspace, justifying the
geometric picture described earlier.

\subsection{Partial Triadic Analysis}

Partial Triadic Analysis (PTA) gives an approach to working with
$L$-table data ``cubes'' \cite{thioulouse2011simultaneous}. That is,
it gives a way of analyzing data of the form $\left(X_{\cdot\cdot l}\right)_{l
  = 1}^{L}$, where each $X_{\cdot\cdot l} \in \reals^{n \times p}$; this is
called a data cube because it can be considered a three-dimensional
array\footnote{sometimes called a tensor when attempting to
  intimidate biologists.} $X \in \reals^{n \times p   \times
  L}$. We denote the $j^{th}$ feature measured on the $i^{th}$ sample
in the $l^{th}$ table by $x_{ijl}$, and the slices over fixed $i$,
$j$, and $l$ by $X_{i \cdot \cdot}$, $X_{\cdot j \cdot}$ and $X_{\cdot
  \cdot l}$. This type of data arises frequently in longitudinal data
analysis, where the same features are collected for the same samples
across a series of $L$ times. The canonical application is in ecology:
we measure the same environmental variables at the same locations at a
set of times. However, the actual ordering of the $L$ tables is not
ever used by this method: in the ecological application, if we
scrambled the time ordering for $L$ tables, the algorithm's result
would not change.

The main idea in PTA is to divide the analysis into two steps,
\begin{enumerate}
  \item Combine the $L$ tables into a single so-called compromise
    table.
  \item Apply any standard method, e.g., PCA, on the compromise table.
\end{enumerate}

Evidently, the only real content in this method is the construction of
the compromise table. A naive approach would simple average each
entry across the $L$ tables. Instead, PTA upweights tables that are
more similar to the average table, as these are considered more
representative. Formally, the compromise is defined as $X^{(c)} =
\sum_{l = 1}^{L}\alpha_{l} X_{\cdot\cdot l} = X\alpha \in \reals^{n \times p}$,
where $\alpha$ (constrained to norm one) is chosen to maximize
$\sum_{l = 1}^{L} \alpha_{l} \left<\overline{X}, X_{\cdot\cdot l}\right>$,
a weighted average of inner-products\footnote{We are using $\left<A,
    B\right> = \tr\left(A^{T}B\right)$.} between each of the $L$
tables and the naive-average table, $\overline{X} = \frac{1}{L}\sum_{l
  = 1}^{L} X_{\cdot\cdot l}$.

The optimal $\alpha$ is easily derived; the Lagrangian of
this optimization is
\begin{align}
\mathcal{L}\left(\alpha, \lambda\right) &= \sum_{l = 1}^{L}
\alpha_{l}\left<\overline{X}, X_{\cdot\cdot l}\right> +
\lambda\left(\|\alpha\|^{2}_{2} - 1\right),
\end{align}
which when differentiated with respect to $\alpha$ gives $\alpha_{l} =
-\frac{1}{2\lambda} \left<\overline{X}, X_{\cdot\cdot l}\right>$. The
constraint that $\|\alpha\|_{2}^{2} = 1$ implies that
$\frac{1}{4\lambda^{2}} \sum_{l^{\prime} = 1}^{L}
\left<\overline{X}, X_{\cdot\cdot l^{\prime}}\right>^{2} = 1$, which gives
$\lambda = \frac{1}{2} \sqrt{\sum_{l^{\prime} =1 }^{L}
  \left<\overline{X}, X_{\cdot\cdot l^{\prime}}\right>^{2}}$, so
$\alpha_{l} =
\frac{\left<\overline{X}, X_{\cdot\cdot l}\right>}{\sqrt{\sum_{l^{\prime}
      =1}^{L}\left<\overline{X},
      X_{\cdot\cdot l^{\prime}}\right>^{2}}}$. Combining these, we can write
the compromise table as
\begin{align}
  X_{c} = \sum_{l = 1}^{L} \frac{\left<\overline{X}, X_{\cdot\cdot l}\right>}{\sqrt{\sum_{l^{\prime}
      =1}^{L}\left<\overline{X},
      X_{\cdot\cdot l^{\prime}}\right>^{2}}} X_{\cdot\cdot l}.
\end{align}

We can try to interpret the compromise matrix geometrically. Suppose
the $X_{\cdot\cdot l}$ define an orthonormal basis; so $\left<X^{l},
  X^{l^{\prime}}\right> = \indic{l = l^{\prime}}$. Then, we can write
the compromise table as
\begin{align}
  X_{c} = \sqrt{L}\sum_{l = 1}^{L}\left<\overline{X},
    X_{\cdot\cdot l}\right>X_{\cdot\cdot l} = \sqrt{L}\overline{X},
\end{align}
a scaled version of the mean.

If however, the tables are not orthonormal, then we place more weight
on directions that are correlated. For example, if $X^{(1)} =
X^{(2)}$, but the rest of the tables are orthogonal to each other and
to these first two tables, then the compromise double counts the
direction $X^{(1)}$. In this sense, $X_{c}$ is different from the
naive-average $\overline{X}$ -- it upweights more highly
represented tables.

We briefly note an alternative, but related, definition of a
compromise table, which seems to have been ignored in the PTA
literature. Instead of maximizing the trace, we can find weights
$\alpha$ to minimize $\|\overline{X} -  \sum_{l =
  1}^{L}\alpha_{l}X_{\cdot\cdot l}\|_{F}^{2}$. This cross-term in this
objective is the same as the weighted average in PTA,
\begin{align}
\|\overline{X} -  \sum_{l =
  1}^{L}\alpha_{l}X_{\cdot\cdot l}\|_{F}^{2}\|_{F}^{2}
&= \|\overline{X}\|_{F}^{2} - 2\sum_{l =
  1}^{L}\alpha_{l}\left<\overline{X}, X_{\cdot\cdot l}\right> + \|\sum_{l =
  1}^{L} \alpha_{l}X_{\cdot\cdot l}\|_{F}^{2} \\
&= \|\overline{X}\|_{F}^{2} - 2\sum_{l =
  1}^{L}\alpha_{l}\left<\overline{X}, X_{\cdot\cdot l}\right> + \sum_{l =
  1}^{L} \alpha_{l}^{2}\|X_{\cdot\cdot l}\|_{F}^{2} + 2 \sum_{l < l^{\prime}}
\alpha_{l}\alpha_{l}^{\prime}\left<X_{\cdot\cdot l}, X_{\cdot\cdot l^{\prime}}\right>.
\end{align}
In contrast to the PTA objective, here there differential penalty on
the $\alpha_{l}$'s that depends on the norms and inner products of the
$X_{\cdot\cdot l}$.

Note that this objective can be optimized by stacking the columns of
$\overline{X}$ and $X_{\cdot\cdot l}$ and performing a regression. That is, set
\begin{align}
y \in \reals^{Ln} &= \begin{pmatrix} \overline{X} \\ \vdots \\
  \overline{X} \end{pmatrix}
\end{align}
and
\begin{align}
X \in \reals^{Ln \times p} = \begin{pmatrix} x_{\cdot 1 1} &
  \dots & x_{\cdot pl} \\ \vdots & & \vdots \\ x{\cdot 1 l}
  & \dots & x_{\cdot p l} \end{pmatrix}.
\end{align}

Perhaps the reason for avoiding this approach is that, compared to the
PTA objective, this approach requires inverting an $Ln \times p$
matrix.

\subsection{Statico and costatis}

In the multivariate ecology literature, it is common to have pairs of
data cubes, where one gives species counts at a set of sites
over time and the other describes quantitative environmental variables
over time. We write these are $X^{(1)}, X^{(2)} \in \reals^{n \times p
  \times L}$. Costatis and Statico are two approaches for analyzing
such data \cite{thioulouse2011simultaneous}. They are easiest to
understand as divide-and-conquer approaches, where the general problem
of analyzing a pair of data cubes is divided into steps adapted to
analyzing single cubes and pairs of (two-dimensional) tables. In
Statico, the paired aspect of the data is dealt with first and
followed by a data cube analysis, while in Costatis, the cube element
of the data are studied first, and followed-up with a paired
analysis. This is described more precisely below.

In Statico, an empirical cross-covariance matrix is constructed at
each time point, $Z^{l} = \frac{1}{n_{l}}X^{(1) T}_{\cdot\cdot
  l}X^{(2)}_{\cdot \cdot l}$. This is, for example, the objective
for a CCA / CoIA for the environmental variables / species count pair
at a specific timepoint $l$. The $L$ matrices $Z^{(l)}$ are then input
into a PTA, yielding a compromise table $Z_{c}$ which can then be
studied with PCA\footnote{But since each $Z^{(l)}$ is a cross-products
  matrix, a PCA on this compromise has the flavor of a being a CoIA.}

Alternatively, in Costatis, a compromise table is constructed for each
of the data cubes $X^{(1)}$ and $X^{(2)}$, using partial triadic
analysis, call these $X^{(1)}_{c}$ and $X^{(2)}_{c}$. These are now
two $n \times p$ matrices, and so can be analyzed by any two-table
dimension reduction method; in Costatis, the standard is to apply CoIA
to the pair $X^{(1)}_{c}$ and $X^{(2)}_{c}$.

Hence, we see that the only difference between these methods is the
order between the CoIA and the PTA. Indeed, this is reflected in the
names of the methods: statis is an abbreviation for a PTA, and Statico
performs a CoIA before a statis while Costatis reverses the order.

\subsection{Reduced-rank regression}
\label{sec:rr-reg}

Reduced-rank regression is an approach to multiresponse regression
which ties the different responses together. Compared to treating each
individual response as its own univariate regression problem, this
pooling across responses can lead to substantial performance
improvements. Further, we will see that the reduced-rank point of view
can aid interpretation as well.

Our setup is that we have collected $p_{1}$ response and $p_{2}$
variables across $n$ features, $y_{i} \in \reals^{p_{1}}$ and $x_{i} \in
\reals^{p_{2}}$, respectively. Our goal is to use this training data to predict
the response $y^{\ast}$ given the $x^{\ast}$ from a new sample. We arrange these
data into two matrices, $Y \in \reals^{n \times p_{1}}$ and
$X \in \reals^{n \times p_{2}}$.

The most straightforwards way to deal with this data is to use least squares
to fit a coefficient matrix $B \in \reals^{p_{2} \times p_{1}}$ relating the
$p_{2}$ features to the $p_{1}$ response coordinates, that is, minimize
$\|Y - XB\|_{F}^{2}$. A slight modification supposes that the responses might
be correlated, and instead optimizes a whitened version of the
problem\footnote{This can also be viewed as using a Mahalanobis metric.},
$\|\left(Y - XB\right)\hat{\Sigma}_{YY}^{-\frac{1}{2}}\|_{F}^{2}$

The optimal $B$ for these two approaches are $\left(X^{T}X\right)^{-1}X^{T}Y$
and $\left(X^{T}\hat{\Sigma}_{YY} X\right)^{-1}X^{T}\hat{\Sigma}_{YY}Y$
respectively. These simply concatenate the coefficients from $p_{1}$ independent
(weighted) linear regressions, one for each response dimension.

This is not a very satisfactory solution, because we would have hoped to share
information across the different response dimensions: we should be able to
improve performance compared to the univariate regressions. Fortunately, this
is often the case in many real applications, because the responses are often
correlated across dimensions. The intuition is that while there may be $p_{1}$
coefficients, the effective dimension of these may be relatively low.
Reduced-rank regression formalizes this with an explicit constraint on the rank
of $B$ \cite{izenman1975reduced, mukherjee2011reduced}. The reduced-rank
regression coefficient $\hat{B}^{\text{rr}}$ is defined as the optimal $B$ in
the following problem,
\begin{align}
\minimize_{B \in \reals^{p_{2} \times p_{1}}} &\|\left(Y - XB\right)\Sigma_{YY}^{-\frac{1}{2}}\|_{F}^{2} \label{eq:rr_obj}\\
\text{such that } &\text{ rank}\left(B\right) \leq K,
\end{align}
for some $K < p_{1}\wedge p_{2}$.

The optimal value is given by $\hat{B}^{\text{ols}}V_{K}V_{K}^{-1}$, where
the columns of $V$ are top $K$ response CCA directions. To see this, consider
the data and parameters in the whitened space,
$Y^{\ast} = Y\hat{\Sigma}_{YY}^{-\frac{1}{2}}$ and
$B^{\ast} = B\hat{\Sigma}_{YY}^{-\frac{1}{2}}$, and rewrite the objective
\ref{eq:rr_obj} as
\begin{align}
\|Y^{\ast} - XB^{\ast}\|_{F}^{2} &= \|Y^{\ast} - \hat{Y}^{\ast \text{ols}}\|_{F}^{2} + \|\hat{Y}^{\ast \text{ols}} - XB^{\ast}\|_{F}^{2},
\end{align}
where we used the fact that the residuals are orthogonal to the column space of
$X$ to remove the cross term. The first term does not involved $B^{\ast}$, so
we can focus on minimizing the second. Now, consider the SVD
$\hat{Y}^{\ast \text{ols}} = \dot{U}\dot{D}\dot{V}^{T}$. We know that one
matrix $A$ of rank $K$ that minimizes
$\|\hat{Y}^{\ast \text{ols}} - A\|_{F}^{2}$ is
$A = \dot{U}_{K}\dot{D}_{K}\dot{V}_{K}^{T} = Y^{\ast \text{ols}}\dot{V}_{k}\dot{V}_{k}^{T}$,
the truncated SVD of $\hat{Y}^{\ast \text{ols}}$, or alternatively its
projection onto the top $K$ right eigenvectors.

In particular, any matrix $B$ that satisfies
\begin{align}
  XB^{\ast} = \hat{Y}^{\ast \text{ols}}\dot{V}_{k}\dot{V}_{k}^{T} = X\hat{B}^{\ast \text{ols}}\dot{V}_{k}\dot{V}_{k}^{T}
\end{align}
solves the reduced rank regression problem, so in particular we can choose
\begin{align}
  \hat{B}^{\text{rr}} &= \hat{B}^{\ast \text{ols}}\dot{V}_{k}\dot{V}_{k}^{T}, \label{eq:brr_coef}
\end{align}
which involves $\hat{B}^{\ast \text{ols}}$ the OLS fit of $Y^{\ast}$ on $X$ and
$V_{k}$, the first $K$ right eigenvectors of the resulting fitted vector
$\hat{Y}^{\ast \text{ols}}$.

There is an interesting connection between this fit and the response canonical
directions of $\hat{Y}$. In particular, consider the eigendecomposition that
follows from the earlier SVD,
\begin{align}
\dot{V}\dot{D}\dot{V}^{T} &=   \hat{Y}^{\ast \text{ols} T}  \hat{Y}^{\ast \text{ols}} \\
&= \left(P_{X}Y\Sigma_{YY}^{-\frac{1}{2}}\right)^{T}\left(P_{X}Y\Sigma_{YY}^{-\frac{1}{2}}\right) \\
  &= \Sigma_{YY}^{-\frac{1}{2}}\Sigma_{YX} \Sigma_{XX}^{-1}\Sigma_{XY}\Sigma_{YY}^{-\frac{1}{2}}. \label{eq:star_ols}
\end{align}

Recall that the response canonical directions $V$ are derived by taking the SVD of
$\Sigma_{XX}^{-\frac{1}{2}}\Sigma_{XY}\Sigma_{YY}^{-\frac{1}{2}} = \tilde{U}\tilde{D}\tilde{V}^{T}$
and setting $V = \Sigma_{YY}^{-\frac{1}{2}}\tilde{V}$. But comparing this to the
form \ref{eq:star_ols}, we find that $\dot{V} = \tilde{V}$, the eigenvectors
from which the CCA response directions are derived are equal to the eigenvectors
of the cross-products of the OLS fits in the whitened space. In particular, we
can write,
\begin{align}
\hat{B}^{\text{rr}} &= \hat{B}^{\ast \text{ols}}\tilde{V}_{K}\tilde{V}_{K}^{T} \\
&= \left(X^{T}X\right)^{-1}X^{T}Y^{\ast}\tilde{V}_{K}\tilde{V}_{K}^{T} \\
&= \left(X^{T}X\right)^{-1}X^{T}Y\Sigma_{YY}^{-\frac{1}{2}}\tilde{V}_{K}\tilde{V}_{K}^{T} \\
&= \left(X^{T}X\right)^{-1}X^{T}Y V_{K} \Sigma_{YY}^{\frac{1}{2}}V_{K}^{T} \\
&= \hat{B}^{\text{ols}}V_{K}V_{K}^{-},
\end{align}

where $V_{K}^{-}$ is the pseudoinverse of the first $K$ response canonical
directions. Hence, the reduced-rank coefficients are just the projection of
the original OLS coefficients onto the subspace spanned by the top $K$ response
canonical directions.

Similarly, we can view directly
$\hat{Y}^{\text{rr}} = X\hat{B}^{\text{rr}} = P_{X}YV_{k}V_{k}^{-1}$, which
means that the reduced-rank fits can be obtained by first projecting the data
columns of $Y$ onto the top $K$ response canonical directions, and then
projecting these pooled $Y$ onto the span of the $X$. If the $Y$ had not
been pooled, then the projection onto the span of the $X$'s is exactly the usual
independent linear regressions described earlier.

\subsection{Modern multivariate methods}

\subsection{Partial Least Squares (PLS)}

PLS is a procedure for sequentially deriving a set of mutually
orthogonal features $\left(z_{k}\right)_{k = 1}^{K}$ which summarize
the relationship between two tables, $X^{(1)}$ and $X^{(2)}$
\cite{wold1985partial}. To obtain the first PLS direction, $z_{1}$,
compute the first left singular vector $u_{1}$ of the cross-covariance
matrix between the two tables, $\hat{\Sigma}_{12} =
\frac{1}{n}X^{(1)T}X^{(2)}$. Then, for each of the $p_{2}$ columns of
$X^{(2)}$, compute the univariate (i.e., partial) regression coefficient
$\hat{\varphi}_{j}$ from the model $u_{1i} = \alpha_{0j} +
\varphi_{j}X^{(2)}_{ij}$, for $i = 1, \dots, j_{1}$. The first PLS
direction is defined as $z_{1} = X^{(2)}\hat{\varphi}_{1}$. To
generate subsequent directions, orthogonalize both $X^{(1)}$ and
$X^{(2)}$ with respect to the current directions, and repeat the process.

This procedure is appealing because, like PCA, it reduces a potentially
high-dimensional matrix $X^{(2)}$ with many correlated columns into a
smaller set of orthogonal directions. Moreover, it achieves this
reduction in a way that accounts for correlation with columns in
x$X^{(1)}$: columns of $X^{(2)}$ that are uncorrelated with $X^{(1)}$
will have no contribution to the PLS directions, even if they account
for a large proportion of variation in $X^{(2)}$.

We have stated the procedure in the form it was originally
proposed; its algorithmic nature makes it difficult to understand
geometrically or probabilsitically. Subsequently, several authors have
attempted to provide statistical interpretations for the PLS
directions. \cite{frank1993statistical} \cite{stone1990continuum} studied
the case where $p_{1} = 1$, so $X^{(1)}$ is a single column
vector. They further suppose the rows of $X^{(1)}$ and $X^{(2)}$ are
drawn iid from distributions $\P^{(1)}$ and $\P^{(2)}$. They found
that the $k^{th}$ PLS direction $z_{k}$ is the $z$ that solves the
optimization
\begin{align}
\max_{z} \medspace & \Corrsubarg{\P^{(1)} \times \P^{(2)}}{x_{i}^{(2) T}z_{k},
x_{i}^{(1)}}\Varsubarg{\P^{(2)}}{z^{T}x_i^{(2)}} \\
\text{such that }&z^{T}X^{(1) T}X^{(1)}z_{j} = 0 \text{ for all }j
\leq k - 1 \text{ and }
\|z\| = 1.
\end{align}
If the covariance term is ommitted, the optimization is identical to
the maximum variance problem that gives the principal component
directions based on $X^{(2)}$. This formulation makes precise the idea
that PLS is a version of principal components that accounts for
correlation with $X^{(1)}$.

An alternative interptetation, due to
\cite{gustafsson2001probabilistic}, is that PLS is the solution to a
particular latent variable model. Suppose $\xi_{i} =
\left(\xi_{i}^{shared}, \xi_{i}^{(2)}\right)$ are drawn iid from a $K_{1} +
K_{2} = K$ dimensional spherical normal. The observed tables $X^{(1)}$
and $X^{(2)}$ have rows drawn iid from
\begin{align}
x_{i}^{(1)} \mid \xi_{i} \sim \Gsn\left(W^{(1)}\xi_{i}^{(shared)} +
  \mu^{(1)}, \sigma^{2}I_{p_{1}}\right) \\
x_{i}^{(2)} \mid \xi_{i} \sim \Gsn\left(W^{(2)}\xi_{i}^{(shared)} + W^{(3)}\xi_{i}^{(2)} +
  \mu^{(2)}, \sigma^{2}I_{p_{2}}\right).
\end{align}

That is, each table is the sum of two components, one that is a
table-specific linear combination is a shared latent variable, and
another that is an arbitrary linear combination of a table-specific
latent variable. The shared feature $\xi^{shared}$ is the object of
interest, and is what PLS implictly estimates.

Compared to classical approaches, modern multivariate methods
are typically designed for more the high-dimensional, heterogeneous
settings. The two methods reviewed in this section are examples of
this trend: partial least squares is well-suited for
high-dimensional spectra (which explains its popularity in
chemometrics), while canonical correspondence analysis was
designed for joint analysis of the heterogeneous continuous / count
data necessary to answer questions in ecology. Unlike traditional
statistical methods, neither approach is explicitly model-based, and
both are iterative, requiring more extensive computation than earlier
techniques.

\subsubsection{Canonical Correspondence Analysis (CCA)}

CCA is a two table method designed to answer questions in ecology
involving joint analysis of count and continuous data. For example
$X^{(1)}$ might be a table whose rows are ecological sites, and whose
columns are the counts of different species at those sites, while
$X^{(2)}$ might describe environmental characteristics at those sites
(e.g., historical rainfall and temperature measurements). A scientific
goal might be to identify those species that are more abundant in
sites with more rainfall or higher temperature. If these environmental
variables were uncorrelated, it would be enough to fit a separate
regression to each; this however is rarely the case, warranting the
need for CCA.

CCA produces low-dimensional representations for both the rows and
columns of $X^{(1)}$ (the sites and species), as well as the latent
subspaces onto which these representations are
projected. Algorithmically, CCA proceeds first constructs the
following matrices, where $1_{r}$ denotes a column vector of $r$ ones,
\begin{enumerate}
  \item An overall frequency matrix,
    \begin{align}
      F =
    \frac{1}{n_{\cdot\cdot}^{(1)}} X^{(1)},
  \end{align}
  where $n_{\cdot\cdot}^{(1)}$ is the sum of all counts in matrix
  $X^{(1)}$.
\item A diagonal matrix of row (site) proportions,
  \begin{align}
    D_{r} &= diag\left(F 1_{p_{1}}\right) \in \reals^{n}.
  \end{align}
\item A diagonal matrix of column (species) proportions,
  \begin{align}
    D_{c} &= diag\left(F^{T}1_{n}\right) \in \reals^{p}.
  \end{align}
\item A projection onto the columns of the environmental matrix
  $X^{(2)}$, reweighting sites according to their species counts,
  \begin{align}
    P_{X^{(2)}} &= D_{r}^{\frac{1}{2}}X^{(2)}\left(X^{(2)
      T}D_{r}X^{(2)}\right)^{-1}X^{(2) T}D_{r}^{\frac{1}{2}} \in
  \reals^{n \times n}.
\end{align}
\end{enumerate}

With these variables set up, compute an SVD,
\begin{align}
D_{r}^{-\frac{1}{2}}\left(F - F
  1_{p}1_{p}^{T}F\right)D_{c}^{-\frac{1}{2}}P_{X} = UDV^{T},
\end{align}
and define row and column scores $Z$ and $Q$ by
\begin{align}
  Z &= D_{r}^{-\frac{1}{2}} UD \\
  Q &= D_{c}^{-\frac{1}{2}}V^{T}D.
\end{align}

There are several ways to interpret this procedure. The original
proposal described CCA as a solution to a fixed-point iteration; they
also demonstrated that this was the approximate MLE of a probability
model \cite{ter1986canonical}. Later, \cite{greenacre1987geometric,
  greenacre1984theory}, provided a geometric view, and
\cite{zhu2005constrained} gave an exact probabilistic
interpretation. Here, we describe the fixed-point iteration
motivation, since it was originating idea for the method, and also the
exact probabilistic interpretation, to continue the theme of
describing implicit probability models behind different algorithmic
dimensionality reduction procedures.

The intuition for the reciprocal averaging procedure is simple: the
scores for different sites should be a weighted average of the species
scores, with larger weights for the species that are more common at
those sites. That is,
\begin{align}
  z_{i} \propto \frac{1}{f_{i\cdot}}\sum_{j = 1}^{p_{1}}f_{ij}q_{ij} \\
  q_{j} \propto \frac{1}{f_{\cdot j}} \sum_{i = 1}^{n} f_{ij}z_{ij},
\end{align}
or, in matrix form,
\begin{align}
Z \propto \diag\left(F 1_{p}\right)^{-1} F Q^{T} \\
Q \propto \diag\left(F^{T} 1_{n}\right)^{-1} P.
\end{align}
This formulation suggests a method to obtain $Z$ and $Q$ --
arbitrarily initialize one and iterate until convergence.

As is, this is not quite the setup that leads to CCA\footnote{It in
  fact gives the solution to the Correspondence Analysis problem (the
  similarity is the reason for the name \textit{Canonical}
  Correspondence Analysis).} -- it doesn't even use information in the
environmental variables table $X^{(2)}$. To recover CCA, a projection
step needs to be inserted before the calculation of row scores,
\begin{enumerate}
\item Arbitrarily initialize $Z$.
\begin{enumerate}
\item Solve $Q^{\prime} \propto \diag\left(F^{T}1_{n}\right)^{-1}F^{T}Z$.
\item Project $Q = P_{X^{(2)}}Q^{\prime}$.
\item Solve $Z \propto \diag\left(Z 1_{p}\right)^{-1} F Q^{T}$.
\end{enumerate}
\end{enumerate}

The iteration now converges to the previously described CCA solution.

A second, less widely appreciated, interpretation, is due to
\cite{zhu2005constrained}. Suppose for now that we are only interested
in a one-dimensional score for rows and columns. Suppose $\alpha$ is a
latent environmental gradient, for example, between warm-dry and
cold-wet sites. For each of the $p_{1}$ species, define a normal
density over the environmental variables,
$f_{j}\left(x_{i}^{(2)}\right) = \Gsn_{p_{(2)}}\left(\mu_{j},
  \Sigma_{j}\right)$. The mode of this density represents the
preferred environmental conditions for species $j$.  Next, project
these densities onto the environmental gradient, giving a univariate
$f_{j}^{\alpha}\left(z_{i}\right) = \Gsn\left(\alpha^{T}\mu_{j},
  \alpha^{T} \Sigma_{j}\alpha\right)$ for each species. The $z_{i}$'s
represent the scores for species $i$ along the environmental gradient
$\alpha$.

The generative model views each species-site pair one at a time. For
each pair involving site $i$ and species $j$ we draw a score according
to $f_{j}^{\alpha}\left(z_{i}\right)$. So, for each site $i$, we draw
a species according to a $p_{1}$-class LDA model.

To use this idea to estimate scores, we need to estimate the
environmental gradient  $\alpha$ (which is also of interest in its own
right). This is done by supposing equal covariances across species,
$\Sigma_{j} = \Sigma$ for all $j$, and finding the $\alpha$ that
maximizes the between vs. total variance across species,
\begin{align}
  \frac{\alpha^{T} \Sigma_{B} \alpha}{\alpha^{T} \Sigma \alpha},
\end{align}
where
\begin{align}
  \Sigma_{B} = \sum_{j = 1}^{p_{1}} f_{\cdot j}\left(\mu_{j} -
    \bar{\mu}\right)\left(\mu_{j} - \bar{\mu}\right)^{T}.
\end{align}

Estimating $\hat{\alpha}$ in this way and writing $z_{i} =
\hat{\alpha}^{T}x_{i}$ gives the original site scores from CCA.

\subsection{Curds \& Whey (C \& W)}

The C\&W procedure is a ``soft'' version of reduced-rank regression,
differentially shrinking the OLS fits with respect to the response
canonical correlation directions, with weights depending on the
associated canonical correlations \cite{breiman1997predicting}. This
is in contrast to reduced-rank regression, whose projection onto the
first $K$ response canonical correlation directions can be viewed as a
hard-thresholding of canonical correlation decomposition. In this way,
C\&W is to reduced-rank regression what ridge regression is to
principal component regression.

More precisely, the C\&W algorithm fits a table $Y$ according to
\begin{align}
  \hat{Y} &= P_{X}YV\Lambda V^{-1}, \label{eq:cw_yhat}
\end{align}
where again $V \in \reals^{p_{1} \times p_{1}}$ are the CCA
directions associated with the response $Y$ and $P_{X}$ is the
projection operator onto the column space of $X$. Here, $\Lambda$ is a
diagonal matrix the determines the degree of shrinkage for the
different canonical directions.

The main difficulty in C\&W is the choice of $\Lambda$, and
\cite{breiman1997predicting} provide several alternatives. One nice
choice can be derived from a generalized cross-validation point of
view; it is nice because shrinkage towards the response canonical
correlation directions emerges automatically, without assuming the
form \ref{eq:cw_yhat} a priori. We describe this approach below.

One way to pool information across the response dimensions is to
define new fitted values from a linear combination of the independent
OLS fits. That is, to predict any the response $y_{i} \in
\reals^{p_{1}}$ for any particular sample, we can set
$\hat{y}^{\text{cw}}_{i} = B\hat{y}^{\text{ols}}_{i}$ for some square
matrix $B \in \reals^{p_{1} \times
  p_{1}}$. But how to choose $B$? One reasonable idea is to choose a
$B$ that has the best performance in a full leave-one-out
cross-validation (LOOCV),
\begin{align}
  \hat{B}^{\text{cw}} := \arg\min_{B} \sum_{ = 1}^{n} \|y_{i} - B
  \hat{y}_{-i}\|_{2}^{2}. \label{eq:cv_looc}
\end{align}

Here, $\hat{y}_{-i}$ is the OLS fitted value for $y_{i}$ when training
on all samples but the $i^{th}$.

It is well known that $\hat{y}_{-i}$ can be decomposed as
\begin{align}
  \hat{y}_{-i} &= \left(1 - g_{i}\right)y_{i} + g_{i} \hat{y}_{i}, \label{eq:cv_looc_trick}
\end{align}
where $g_{i} = \frac{1}{1 - h_{ii}}$ and $h_{ii}$ are the diagonal
elements of the projection matrix $\hat{y} = Hy$ and $\hat{y}_{i}$ is
the fitted value on the $i^{th}$ sample, both computed using all the
data.

The advantage of the representation \ref{eq:cv_looc_trick} is that it
makes it possible to perform a full LOOCV without actually computing
$n$ regression models: only a single full regression using all the
data needs to be computed, and all the LOOCV fits $\hat{y}_{-i}$ can
be deduced directly. This can dramatically speed up computation of
\ref{eq:cv_looc}.

This does not resolve the question of how to manage the search over
possible $B$; but concrete proposals are discussed in
\cite{breiman1997predicting}.

An alternative is to use generalized cross-validation (GCV). This
approach suggests a form for $B$ based on the response canonical
correlation directions. The GCV approximation is that the $h_{ii}$ can
be approximated by their average across all diagonal elements of $H$:
$h_{ii} \approx h := \frac{1}{n}\tr\left(H\right)$ for all $i$. In the
same spirit, define $g = \frac{1}{1 - h}$, and approximate
\begin{align}
  \hat{y}_{-i} \approx \left(1 - g\right)y_{i} + g\hat{y}_{i}.
\end{align}

Then, the LOOCV error optimized in \ref{eq:cv_looc} can be
simplified to
\begin{align}
  \sum_{i = 1}^{n}\|y_{i} - B\hat{y}_{-i}\|_{2}^{2} &= \sum_{i =
    1}^{n} \|y_{i} - B\left(\left(1 - g\right)y_{i} +
    g\hat{y}_{-i}\right)\|_{2}^{2},
\end{align}
and differentiating with respect to $B$, we find that the optimal
$\hat{B}^{\text{cw}}$ in this GCV framework must satisfy
\begin{align}
\sum_{i = 1}^{n}\left(y_{i} - B\left(\left(1 - g\right)y_{i} +
    g\hat{y}_{-i}\right)\right)\left(\left(1 - g\right)y_{i} +
  g\hat{y}_{-i}\right)^{T},
\end{align}
or equivalently
\begin{align}
\sum_{i = 1}^{n} y_{i}\left(\left(1 - g\right)y_{i} +
  g\hat{y}_{-i}\right)^{T} &=  \sum_{i = 1}^{n}B\left(\left(1 -
    g\right)y_{i} + g\hat{y}_{-i}\right)\left(\left(1 - g\right)y_{i}
  +  g\hat{y}_{-i}\right)^{T},
\end{align}
which in matrix form is
\begin{align}
\left(1 - g\right)Y^{T}Y + g\hat{Y}^{T}Y = B\left(\left(1 - g\right)Y
  + g \hat{Y}\right)^{T}\left(\left(1 - g\right)Y + g \hat{Y}\right), \label{eq:gcv_mat_form}
\end{align}
where $\hat{Y} \in \reals^{n \times p_{1}}$ has $i^{th}$ row
$\hat{y}_{-i}$.

Now, we can represent these cross-products in a way that is suggestive
of CCA,
\begin{align}
  Y^{T}Y &= n \hat{\Sigma}_{YY} \\
  \hat{Y}^{T}Y &= Y^{T}HY = Y^{T}X\left(X^{T}X\right)^{-1}X^{T}Y =
  n\hat{\Sigma}_{YX}\hat{\Sigma}_{XX}^{-1} \hat{\Sigma}_{XY} \\
  \hat{Y}^{T}\hat{Y} &= Y^{T}H^2 Y = Y^{T}HY=
  n\hat{\Sigma}_{YX}\hat{\Sigma}_{XX}^{-1} \hat{\Sigma}_{XY},
\end{align}
where we used that $H^{2} = H$, since it is a projection
matrix. Substituting this notation into \ref{eq:gcv_mat_form} and
ignoring the scaling $n$ yields
\begin{align}
\left(1 - g\right)\hat{\Sigma}_{YY} + g
\hat{\Sigma}_{YX}\hat{\Sigma}_{XX}^{-1}\hat{\Sigma}_{XY} &=
B\left[\left(1 - g\right)\hat{\Sigma}_{YY} + \left(2g -
    g^{2}\right)\hat{\Sigma}_{YX}\hat{\Sigma}_{XX}^{-1}\hat{\Sigma}_{XY}\right].
\end{align}
Postmultiplying by $\hat{\Sigma}_{YY}$ gives
\begin{align}
  \left(1 - g\right)I_{p_{1}} + g\hat{Q}^{T} &= B\left[\left(1 -
      g\right)I_{p_{1}} + \left(2g -
      g^{2}\right)\hat{Q}^{T}\right], \label{eq:cca_gcv_id}
\end{align}
where,
\begin{align}
\hat{Q} :=
\hat{\Sigma}_{YY}^{-1}\hat{\Sigma}_{YX}\hat{\Sigma}_{XX}^{-1}\hat{\Sigma}_{XY}
\in \reals^{p_{1}\times p_{1}}.
\end{align}
Now, we claim that we can decompose $\hat{Q} = VD^{2}V^{-1}$, where $V
\in \reals^{p_{1} \times p_{1}}$ is the full matrix of CCA response
directions,  $D$ is diagonal with the canonical correlations. Indeed,
the usual CCA response directions $V$ can be recovered by setting $V =
\hat{\Sigma}_{YY}^{-\frac{1}{2}}\tilde{V}$, where $\tilde{V}$ comes
from the SVD of $A :=
\Sigma_{XX}^{-\frac{1}{2}}\Sigma_{XY}\Sigma_{XX}^{-\frac{1}{2}} =
\tilde{U}D\tilde{V}^{T}$. We then note that
\begin{align}
  Q &= \Sigma_{YY}^{-\frac{1}{2}}A^{T}A\Sigma_{YY}^{\frac{1}{2}} \\
  &=
  \Sigma_{YY}^{-\frac{1}{2}}\tilde{V}^{2}D^{2}\tilde{V}^{T}\Sigma_{YY}^{\frac{1}{2}}\\
  &= VD^{2}V^{-1},
\end{align}
where we are able to write $V^{-1} =
\tilde{V}^{T}\Sigma_{YY}^{\frac{1}{2}}$ because $\tilde{V}$ is the
full (untruncated) matrix of eigenvectors, so $\tilde{V}\tilde{V}^{T}
= I$ in addition to the usual $\tilde{V}^{T}\tilde{V} =I$, which holds
even for the truncated SVD.

For now, take this for granted, then \ref{eq:cca_gcv_id} can be
expressed as
\begin{align}
  V^{-T}\left[\left(1 - g\right)I_{p_{1}} + gD^{2}\right]V^{T} &=
  BV^{-T}\left[\left(1 - g\right)I_{p_{1}} + \left(2g -
      g^2\right)D^{2}\right]V^{T}
\end{align}
so, the $B$ satisfying the normal equations has the form
\begin{align}
\hat{B}^{\text{cw}} &= V^{-T}\Lambda V^{T},
\end{align}
where $\Lambda$ is a diagonal matrix with entries
\begin{align}
\lambda_{jj} = \frac{1 - g + d_{jj}^{2}g}{1 - g + \left(2g -
    g^{2}\right)d_{jj}^{2}}.
\end{align}
Notice that when $n$ is large, $\frac{1}{n}\tr H$ will be small, so $g
\approx 0$ and less shrinkage will be applied.

Recall that $\hat{B}^{\text{cw}}$ is used to pool across OLS fits,
$\hat{y}_{i}^{\text{cw}} = \hat{B}^{\text{cw}}\hat{y}_{i}^{\text{ols}}$. That is,
\begin{align}
\hat{Y}^{\text{cw}} &= \hat{Y}^{\text{ols}}B^{T} =
\hat{Y}^{\text{ols}}V\Lambda V^{-1}
\end{align}
which we can also view as $\hat{Y}^{\text{cw}}V =
\left(\hat{Y}^{\text{ols}}V\right)\Lambda$. This means that the
C\&W coordinates along the canonical directions $V$ are set as the OLS
fits $\hat{Y}^{\text{ols}}$ along the canonical directions $V$, with
weights defined by $\Lambda$. The actual $\hat{Y}^{\text{cw}}$ are
recovered by transforming back to the original coordinate system. A
similar way to view the C\&W fits is $\hat{Y}^{\text{cw}}V =
P_{X}\left(YV\right)\Lambda$. From this point of view, we shrink the
original data $Y$ according to the canonical directions, and then
project the shrunk data onto the subspace defined by the columns of
$X$. In any case, we see that C\&W pools across regression problems
through a soft shrinkage weighted along canonical response
directions.

\subsection{Methods from Machine Learning}

Several different literatures within machine learning have each
developed approaches to dimensionality reduction across multiple
tables, each relying on the tools studied in that literature. In this
section, we will study some methods that have arisen in the kernel
learning, spectral clustering, regularized modeling, and probabilistic
inference communities. A common theme across these methods is that
they provide the scientist flexiblity in specifying the form of the
data model; these literatures emphasize tools that can be adapted to
the structure of a specific dataset, rather than algorithms that
should be applied generally. However, as these methods have typically
not been applied to ecological or microbiome studies, it is not yet
clear what their overall utility will be.

\subsubsection{Kernel Canonical Correlation Analysis (KCCA)}

KCCA is a variant of CCA designed to be sensitive to nonlinear
associations across tables \cite{akaho2006kernel, bach2003kernel}. It
does this by implicitly lifting the original
data to a richer feature space, with the hope that nonlinear
associations in the original space become linear associations in the
richer space. Informaly, KCCA is the algorithm that emerges after
applying the kernel trick to CCA.

More formally, let $\varphi^{\left(l\right)}: \reals^{p_{l}}\mapsto
H^{(l)}$ be a mapping from the space associated with table $l$ to a
richer Hilbert space. For example, $\varphi^{l}$ might map vectors
into an expansion of all polynomial products of coordinates, up to
some fixed kernel; this is mapping is called the polynomial
kernel. As in CCA, let $\P^{(1)}$ and $\P^{(2)}$ denote the sampling
distributions associated with the two tables; let $x_{i}^{(1)}$ and
$x_{i}^{(2)}$ denote generic draws from these tables.

In the same way that the first CCA direction maximizes the covariance
between linear combinations $c_{1}^{(1) T}x_{i}^{(1)}$ and $c_{1}^{(2)
  T}x_{i}^{(2)}$ (subject to a variance constraint), KCCA maximizes
the correlation between linear functionals, $z_{i1}^{(1)} =
\left<c_{1}^{(1)}, x_{i}^{(1)}\right>$ and $z_{i1}^{(2)} =
\left<c_{1}^{(2)}, x_{i}^{(2)}\right>$.
\begin{align}
  \argmax_{c_{1}^{(1)} \in H^{(1)}, c_{1}^{(2)} \in H^{(2)}}
  &\Covsubarg{\P^{(1)} \times \P^{(2)}}{z_{i1}^{(1)}, z_{i1}^{(2)}} \\
\text{subject to }\Varsubarg{P^{(1)}}{z_{i1}^{(1)}} &=
\Varsubarg{P^{(2)}}{z_{i1}^{(2)}} = 1. \label{eq:optim_kcca}
\end{align}

As is, the problem is not well-posed, and it is necessarily to
regularize. The regularized Lagrangian associated with the
optimization \ref{eq:optim_kcca} is
\begin{align*}
&\Covsubarg{\P^{(1)} \times \P^{(2)}}{z_{i1}^{(1)}, z_{i1}^{(2)}} - \\
&\frac{\rho^{(1)}}{2}\Varsubarg{P^{(1)}}{z_{i1}^{(1)}} -
\frac{\rho^{(2)}}{2}\Varsubarg{P^{(2)}}{z_{i1}^{(2)}} \\+
&\frac{\lambda^{(1)}}{2}Pen\left(c_{1}^{(1)}\right) +
\frac{\lambda^{(2)}}{2}Pen\left(c_{1}^{(2)}\right) \label{eq:lagrangian_kcca}
\end{align*}

Now, note that the optimal $c_{1}^{(1)}$ and $c_{1}^{(2)}$ must lie in
the spans of $\left(\varphi^{(1)}\left(x_{i}\right)\right)_{i =1
}^{n}$ and $\left(\varphi^{(2)}\left(x_{i}\right)\right)_{i =1 }^{n}$, respectively,
since directions orthogonal to this subspace cannot improve the
correlation in the objective. Therefore, we can write
\begin{align}
c_{1}^{(1)} &= \Phi^{(1)} \alpha^{(1)} \\
c_{1}^{(2)} &= \Phi^{(2)} \alpha^{(2)},
\end{align}
where $\Phi^{(l)} \in \reals^{n \times
  \text{dim}\left(H^{(l)}\right)}$ has $i^{th}$ row
$\varphi^{(l)}\left(x_{i}\right)$.

Substituting this into the Lagrangian \ref{eq:lagrangian_kcca}, it
becomes clear that only the cross-products $\Phi^{(1) T}\Phi^{(1)}$,
and $\Phi^{(2) T}\Phi^{(2)}$ appear. Since these inner products can be
written as kernel matrices $K^{(l) (l)} \in \reals^{n \times n}$ with
entries $\varphi^{(l)
  T}\left(x^{(1)}_{i}\right)\varphi^{\left(l\right)}\left(x^{(2)}_{j}\right)$,
we have that the optimization can be fully kernelized.  It can then be
shown that the optimal $\alpha^{(1)}$ and $\alpha^{(2)}$ are the
solutions to the generalized eigenvalue problem,
\begin{align}
  \begin{pmatrix}
    0 & K^{(1)}K^{(2)} \\
    K^{(2)}K^{(1)} & 0
  \end{pmatrix}
  \begin{pmatrix}
    \alpha^{(1)} \\
    \alpha^{(2)}
  \end{pmatrix}
&= \rho
\begin{pmatrix}
  \left(K^{(1)} + \lambda^{(1)}I_{p_{1}}\right)^{2} & 0 \\
  0 & \left(K^{(2)} + \lambda^{(2)}I_{p_{2}}\right)^{2}
\end{pmatrix}
\begin{pmatrix}
  \alpha^{(1)} \\
  \alpha^{(2)}
\end{pmatrix}.
\end{align}

The contribution of \cite{yamanishi2003extraction} was to extend this
procedure to more than 2 tables. The approach is similar to the idea
of average pairwise covariance, discussed in the section about
CCA. Formally, if we let $z_{i1}^{(l)} = \left<c_{1}^{(l)|},
  x_{i}^{(l)}\right>$ and try to optimize

\begin{align*}
&\sum_{l, l^{\prime} = 1}^{L}\Covsubarg{\P^{(l)} \times \P^{(l^{\prime})}}{z_{i1}^{(l)}, z_{i1}^{(l^{\prime})}} -
\sum_{l = 1}^{L}\frac{\rho^{(l)}}{2}\Varsubarg{P^{(l)}}{z_{i1}^{(l)}} -
 \sum_{l = 1}^{L} \frac{\lambda^{(1)}}{2}Pen\left(c_{1}^{(l)}\right) \label{eq:lagrangian_mkcca}
\end{align*}

Replacing the inner products $\Phi^{(l)}\Phi^{(l)}$ as before leads to
an analogous generalized eigenvalue problem,
\begin{align}
\begin{pmatrix}
0 & \dots & K_{1}K_{L} \\
\vdots & \ddots & \vdots \\
K_{L}K_{1} & \dots  & 0
\end{pmatrix}
\begin{pmatrix}
  \alpha_{1} \\
  \vdots \\
  \alpha_{L}
\end{pmatrix} &=
\rho \begin{pmatrix}
  \left(K_{1} + \lambda_{1}I_{p_{1}}\right)^{2} & \dots & 0 \\
  \vdots & \ddots & \vdots \\
  0 & \dots & \left(K_{L} + \lambda_{L}I_{p_{L}}\right)^{2}
\end{pmatrix}
\begin{pmatrix}
  \alpha_{1} \\
  \vdots \\
  \alpha_{L}
\end{pmatrix}
\end{align}
whose solution is called ``multiple kernel canonical correlation
analysis'' by \cite{yamanishi2003extraction}.

A geometric interpretation of Kernel CCA was described in
\cite{kuss2003geometry}. The core idea is to translate the Euclidean
picture associated with CCA to the more general RKHS setting. Often,
however, KCCA results can be difficult to study, because the only
output are the scores relating samples (eigenvectors are never
computed in the Hilbert spaces $H^{(l)}$, so it's impossible to make a
biplot). However, the KCCA scores can be interpreted using
supplementary features, as in any other dimensionality reduction
method.

An complementary formulation of Kernel CCA was presented in
\cite{lanckriet2004statistical} (todo: maybe describe this approach
too?).

\subsubsection{Penalized Matrix Decomposition}

In high-dimensional settings, sparsity is a desirable property, both
for interpretability and statistical stability. For example, in the
regression setting, an $\ell^{1}$-penalty can be used to ensure that
only a few coefficients are nonzero. A regression model using only a
few features is easier to understand than one involving a linear
combination of all possible features. Further, regularized models
typically outperform their non-regularized counterparts; in fact, it
is impossible to fit a non-regularized linear regression when the
number of features is greater than the number of samples.

The Penalized Matrix Decomposition (PMD) is a general approach for
adapting the reguarlization machinery developed around regression to
the multivariate analysis setting
\cite{witten2009penalized}. Furthermore, a CCA and MultiCCA instance
of the PMD has been well-studied \cite{witten2009penalized,
  witten2013package}.

The general setup is as follows. Suppose we only want a
one-dimensional representation of samples, and are basing analysis on
a single matrix $X \in \reals^{n \times p}$. Recall that the first
$k$-eigenvectors recovered by PCA span a subspace that minimizes the
$L^{2}$ distance from the original data (rows of $X$, viewed as points
in $\reals^{p}$) to their projection onto that subspace. In
particular, when $k = 1$, the associated PCA coordinates $u \in
\reals^{n}$ and eigenvector $v$ can be \textit{defined} as the optimal
values in the problem
\begin{align}
  \minimize_{u \in \reals^{n}, v \in \reals^{p}, d \in \reals} &\|X - duv^{T}\|_{2}^{2} \\
  \text{subject to } &\|u\|_{2}^{2} = \|v\|_{2}^{2} = 1.
\end{align}

The PMD generalizes this formulation of rank-one PCA to enforce
structure (e.g., sparsity) on $u$ and $v$. The PMD solutions $u$ and
$v$ are defined to be the optimizers of
\begin{align}
\label{eq:pmd_opt} \minimize_{u \in \reals^{n}, v \in \reals^{p}, d
  \in \reals} &\|X - duv^{T}\|_{2}^{2} \\
  \text{subject to } &\|u\|_{2}^{2} = \|v\|_{2}^{2} = 1 \\
  & \text{Pen}_{1}\left(u\right) \leq mu_{1} \\
  & \text{Pen}_{2}\left(v\right) \leq \mu_{2}.
\end{align}

This only defines one dimensional approximations. To obtain a sequence
of scores / eigenvectors $\left(u_{k}\right)_{k = 1}^{K}$ and
$\left(v_{k}\right)_{k = 1}^{K}$, define $u_{k}$ and $v_{k}$ as the
optimizers of the problem \ref{eq:pmd_opt} on the residual $X^{k}$
obtained by subtracting out the previous approximation: $X^{k} := X^{k
- 1} - d_{k - 1}u_{k - 1}v_{k - 1}^{T}$ where $d_{k} = u_{k}^{T}
X^{k}v_{k}$ and $X^{1} = X$.

(todo: geometric interpretation)
(todo: generalized least squares / probabilistic interpretation
\cite{allen2014generalized})

This view can be specialized to develop regularized versions of a
number of multivariate analysis problems; here consider applications
to the CCA and Multiple CCA\footnote{CCA with $L$ tables, as in
  multiple kernel CCA above.} problems.

Using the fact that $\|A\|_{F}^{2} = \tr\left(A^{T}A\right)$ along
with the linearity and the cyclic property of the trace, the objective
in \ref{eq:pmd_opt} can be rewritten (using $\equiv$ to mean equality
up to terms constant in $u$ and $v$),
\begin{align}
  \|X - duv^{T}\|_{F}^{2} &\equiv \tr\left(\left(X -
      duv^{T}\right)^{T}\left(X - duv^{T}\right)\right) \\
  &\equiv -2d\tr\left(X^{T}uv^{T}\right) + d^{2}tr\left(uv^{T}vu^{T}\right) \\
  &\equiv -2d v^{T}X^{T}u + d^{2}
\end{align}
where for the last equivalence we used that $v^{T}v = u^{T}u = 1$.

From this expression, and by partially minimizing out $d =
v^{T}X^{T}u$, we see that the PMD solutions $u$ and $v$ in
\ref{eq_pmd_opt} can be found as the optimizers of
\begin{align}
\label{eq:pmd_reform}  \maximize_{u \in \reals^{n}, v \in \reals^{p}} &u^{T}X^{T}v \\
  \text{subject to } &\|u\|_{2}^{2} = \|v\|_{2}^{2} \\
  &\text{Pen}_{1}\left(u\right) \leq \mu_{1} \\
  &\text{Pen}_{2}\left(v\right) \leq \mu_{2}
\end{align}

Notice that, as long as the penalties are convex in $u$ and $v$, the
optimization is biconvex, so a local maximum can be found by
alternately maximizing over $u$ and $v$.

It is not in general obvious how to choose the regularization
parameters $\mu_{1}$ and $\mu_{2}$. For example, if the penalties are
$\ell^{1}$-norms, then these parameters would control the overall
sparsity of the underlying factors. The proposal in
\cite{witten2009penalized} was to apply cross-validation on the
reconstruction errors after holding out random entries in $X$.

From this form, we can derive a PMD, sparsity-inducing version of
CCA. Recall the maximal-covariance interpretation of CCA,
\begin{align}
  \maximize_{c_{1}^{(1)} \in \reals^{p_{1}}, c_{1}^{(2)} \in
    \reals^{p_{2}}} &\Covsubarg{\P^{(1)}, \P^{(2)}}{c_{1}^{(1)
      T}x_{i}^{(1)},
    c_{1}^{(2) T} x_{i}^{(2)}}, \\
  &\Varsubarg{\P^{(1)}}{x_{i}^{(1)}} = \Varsubarg{\P^{(2)}}{x_{i}^{(2)}} = 1,
\end{align}
which in practice is approximated by
\begin{align}
  \maximize_{c_{1}^{(1)} \in \reals^{p_{1}}, c_{1}^{(2)} \in
    \reals^{p_{2}}} & c_{1}^{(1) T}\hat{\Sigma}_{12}c_{1}^{(2)} \\
  \text{subject to } &c_{1}^{(1) T}\hat{\Sigma}_{11} c_{1}^{(1)} =
  c_{1}^{(2) T}\hat{\Sigma}_{22}c_{1}^{(2)} = 1.
\end{align}

\cite{witten2009penalized} argues for diagonalized CCA, in which the
variance constraints are replaced by unit norm constraints, and
sparsity-inducing $\ell^{1}$ constraints are added,
\begin{align}
  \maximize_{c_{1}^{(1)} \in \reals^{p_{1}}, c_{1}^{(2)} \in
    \reals^{p_{2}}} & c_{1}^{(1) T}\hat{\Sigma}_{12}
  c_{1}^{(2)} \\
  \text{subject to } &\|c_{1}^{(1)}\|_{2}^{2} = \|c_{1}^{(2)}\|_{2}^{2} = 1 \\
  &\|c_{1}^{(1)}\|_{1} \leq \mu_{1} \\
  &\|c_{1}^{(2)}\|_{1} \leq \mu_{2}
\end{align}
which is exactly in the form \ref{eq:pmd_reform} where $X =
\hat{\Sigma}_{12}$, so indeed the PMD framework includes this form of
sparse CCA.

Multiple CCA can also be described in this framework, by replacing
the objective with the sum over all pairwise covariances, $\sum_{l,
  l^{\prime} = 1}^{L} c_{1}^{(l) T}X^{(l)
  T}X^{(l^{\prime})}c_{1}^{(l)^{\prime}}$, and introducing constraints
for each of the $c_{1}^{(l)}$.

\subsubsection{Covariate-assisted Spectral Clustering}

Spectral clustering is an approach to clustering samples using ideas
from graph partitioning. A general question is whether supplmental
data sources can be incorporated into a spectral clustering, while
keeping the similarities defined by some primary dataset. The
community has investigated several possibilities
\cite{binkiewicz2014covariate, jiang2012transfer}; we will describe
the approach in \cite{binkiewicz2014covariate}.

Recall the spectral clustering algorithm. The first step is to build a
weighted graph between all samples, where the edge weights give a
similarity between the connected pair of samples, and call the
associated adjacency matrix $A \in \reals^{n \times n}$. For example,
a common choice is a Gaussian radial basis, $A_{ij} =
\exp{-\frac{1}{2\sigma^{2}}\|x_{i} - x_{j}\|_{2}^{2}}$ for some
variance $\sigma^{2}$; sometimes entries $A_{ij}$ smaller than $\eps$
are thresholded to zero, to take advantage of computational speedups
involved with sparse matrices. Let $D \in \reals^{n \times n}$ be a
diagonal matrix whose elements sum the total edge-weight emanating
from individual nodes, that is, $D_{ii} = \sum_{j = 1}^{n}
A_{ij}$. Take the top $K$ eigenvectors\footnote{$K$ is the parameter
  for the number of clusters to use, it must be chosen ahead of time.}
of $L = D^{-\frac{1}{2}}A D^{-\frac{1}{2}}$, call them $U \in
\reals^{n \times k}$. Then, normalize the rows of $U$ to unit length,
and cluster these rows using $K$-means. These are the final clusters
reported by spectral clustering. Note that, in practice, it is common
to ``regularize'' $L$ by replacing $D$ with $D + \tau I_{n}$, where
$\tau$ is a new tuning parameter.

The essential step is the transformation to the space of eigenvectors
of $L$. A nice interpretation of this transformation is think back to
the weighted graph associated with $A$. Consider a random walk on this
graph, which transitions from $i$ to to some $j \in \{1, \dots, n\}$
with probability proportional to $A_{ij}$; this transition probability
is exactly $L_{ij}$. Suppose the graph associated with $A$ has $K$
separate components, which we write as $V = \cup_{k = 1}^{K}
V\left(K\right)$. Then there are $K$ stationary distributions for this
random walk, with the $k^{th}$ having support confined to
$G\left(k\right)$. In particular, the vectors $u_{k}$ having $i^{th}$
entry $\indic{i \in G\left(k\right)}$ are eigenvectors of $L$, each
with eigenvalue 1. Hence, clustering based on the top $K$ eigenvectors
will assign nodes based on their pattern of $0$'s and $1$'s, which
corresponds to the components $G\left(k\right)$. The idea in spectral
clustering is that, even if the graph is not exactly split across $K$
components, if it is approximately split, then a similar pattern in
the eigenvectors will emerge, and it will still be possible to cluster
them.

Note that spectral clustering is a two-step procedure, the first finds
a low-dimensional representation that can be easily clustered, while
the second performs the actual clustering in this space. To recover
ordinations comparable to those describe elsewhere, it is possible to
stop after the first step, this is sometimes called a ``diffusion
map'' \cite{coifman2005geometric}.

From a high-level, the approach in \cite{binkiewicz2014covariate} is
to mix the transition matrix $L$ obtained from the original data with
a similarity defined independently from the supplementary
data. Specifically, first compute $L$ from similarities based on rows
of $X^{(1)}$. Next, compute the top $K$ eigenvectors $U \in \reals^{n
  \times K}$ of
\begin{align}
  LL^{T} + \alpha X^{(2)}X^{(2) T},
\end{align}
and then row-normalize and cluster as before. Extending the graph
interpretation from before, the matrix $LL^{T}$ can be interpreted as
a two-step transition matrix based on similarities in
$X^{(1)}$. The matrix $X^{(2)}X^{(2) T}$ is an unnormalized empirical
covariance between rows of $X^{(2)}$, and adding it to $LL^{T}$
modifies the transition probabilities between nodes using the
supplemental data. The parameter $\alpha$ trades off the importance of
$X^{(1)}$ vs. $X^{(2)}$ in computing these similarities.

\subsection{Multitask learning}

Some of the ideas pioneered in reduced-rank regression and the curds-and-whey
algorithm have experienced a revival in the machine learning and artificial
intelligence communitiies, under the name multitask learning
\cite{caruana1997multitask, thrun2012learning}. These developments
were motivated by problems in robotics and computer vision, and unlike earlier
statistical methods, rely more heavily on iterative optimization and sampling
rather than analytical solutions.

Unlike previous sections in this review, multitask learning is not an individual
method but rather a general approach to sharing information across prediction
problems, and there is a substantial literature exploring specific techniques.
To ground the discussion, we review the main contributions from
\cite{zhang2005learning, chen2010graph, argyriou2008convex}, though these by
no means represent the breadth of techniques explored in multitask learning.

\subsubsection{Bayesian multitask learning}

\cite{zhang2005learning} formulates a general probabilistic approach to
multitask learning. Their framework encapsulates both regression and
classification, for simplicity we specialize to regression only. Like
reduced-rank regression and C\&W, the main idea is to pool the coefficients
across otherwise separate regression problems; here this is accomplished by
prior specification.

The data come from $p_{1}$ tasks. Within the $r^{th}$ task, we have features
$x_{i}^{(r)} \in \reals^{p_{2}}$ and responses responses
$y_{i}^{(r)} \in \reals$ for the $i^{th}$ sample. Note that the reduced-rank
and C\&W procedures both assume that the features $x_{i}^{(r)}$ don't vary
across the regression problems; this problem is applies to a wider range of
problems. We use the shorthand
$D = \left\{\left(x_{i}^{(r)}, y_i^{(r)}\right)_{i = 1}^{n_{r}}\right\}_{r = 1}^{p_{1}}$.

The responses are assumed drawn i.i.d. from a linear model with
problem-specific coefficient $\beta^{(r)}$,
\begin{align}
y_{i}^{(r)} \sim \Gsn\left(x_{i}^{(r) T}\beta^{(r)}, \sigma^{2}\right).
\end{align}
To tie the problems together, the $\beta^{(r)}$ are modeled using
$K$-dimensional latent factors,
\begin{align}
  \beta^{(r)} \sim \Gsn\left(S w^{(r)}, \Psi\right), \label{eq:beta_r_zhang}
\end{align}
for some latent source matrix $S \in \reals^{p_{2} \times K}$ and mixing
weights $w^{(r)}$. Different models come from choosing different priors on
$w^{(r)}$; for now suppose the $w^{(r)}$ are drawn jointly from
\begin{align}
w^{(1)}, \dots, w^{(r)} \sim  p_{\Phi}\left(\left(w^{\left(r\right)}\right)_{r = 1}^{j_{1}}\right).
\end{align}

The nonrandom parameters in this model are
$\theta := \{\sigma^{2}, S, \Psi, \Phi\}$. The weights and coefficients
$w^{(r)}$ and $\beta^{(r)}$ are random parameters; for inference they will
be treated as latent data.

Note that the form of the $\beta^{(r)}$ in \ref{eq:beta_r_zhang} can
equivalently be written as
\begin{align}
\beta^{(r)} &= Sw^{(r)} + \eta^{(r)},
\end{align}
where $\eta^{(r)} \sim \Gsn\left(0, \Psi\right)$. This clarifies what the latent
factor model form of $\beta^{(r)}$ does: it decomposes the slope into a part
that shares across all models, via $Sw^{(r)}$, and a part that is specific to
the $r^{(th)}$ regression problem. Hence, different priors on
  $\left(w^{(r)}\right)$ allow the modeler to trade-off the degree of sharing
  across regression problems: setting $w^{(r)} \equiv 0$ decomposes the model
  into independent regression problems, while enforcing that they have large
  variance will ensure that they dominate the problem-specific component
  $\eta^{(r)}$.

The authors propose special cases corresponding to particular choices for the
prior $p_{\Phi}\left(\left(w^{(r)}\right)_{r = 1}^{j_{1}}\right)$.

\begin{enumerate}
  \item $w^{(r)} \equiv 0$ decomposes the problem into independent regressions.
  \item $w^{(r)} = 1$ turns the model into a "contaminated-signal"
    model\footnote{Note the similarity to random effects.}, of the form
    $\beta^{(r)} \sim \Gsn\left(\mu, \Psi\right)$.
  \item Drawing $w^{(r)} \sim \Mult\left(1, \Phi\right)$ clusters the regression
    problem into subproblems that each share the same coefficient.
  \item Drawing $w^{(r)} \sim \text{Lap}\left(0, \Phi\right)$
    encourages sparse weights. Though not technically a part of the
    specification above, it is possible to put a similar prior on $S$
    to induce sparsity on the shared sources.
  \item Drawing $w^{(r)} \sim DP\left(\alpha, G_{0}\right)$ has a similar effect
    as the clustered-regressions model, though now the number of clusters does
    does not need to be explicitly specified in advance\footnote{The parameter
      $\alpha$ does modulate the general number of clusters identified,
      however.}.
  \item Draw the $w^{(r)}$ jointly according to a dynamical system, for example
    $w^{(r)} = \Phi w^{(r - 1)} + \eps_{r}$. Unlike the previous
    descriptions, which draw the $w^{(r)}$ independently across
    regression problems, this view can be used to model the evolution
    of the regression coefficients. In general, prior information on
    how the coefficients should be related can be incorporated by
    correlating the $w^{(r)}$ across regression problems.
\end{enumerate}

To perform inference, the author employ a variational strategy. While inference
of the true posterior jointly over
$\left(z^{(r)}\right)_{r = 1}^{p_{1}} := \left(\beta^{(r)}, w^{(r)}\right)_{r = 1}^{p_{1}}$ is
generally intractable\footnote{An exception is when $w^{(r)}$ are given standard
  multivariate normal priors, in which case normal-normal conjugacy can be
  used.}. However, it is possible to search for a distribution
$q^{\ast}\left(\left(z^{(r)}\right)_{r = 1}^{p_{1}}\right)$ such that,
informally, the likelihood of the observed data is large when $z \sim q$. More
formally, we can find a point $q^{\ast}$ in a variational family
$\left(q_{\gamma}\left(\cdot\right)\right)_{\gamma \in \Gamma}$ such that
the expected complete data likelihood is large\footnote{The expected complete
  data likelihood would be maximized by setting $q$ to the true posterior, but
  as this is intractable, we have restricted attention to a smaller variational
  family, which we hope is not too "far" from the true posterior.}, and this
$q^{\ast}$ can be serve as a proxy for the true posterior.

We try now to explain this more concretely. We think of the random parameters
$z^{(r)}$ as latent data. If they were known, the loglikelihood of fixed
parameters $\theta$ would have a simple form; as is, it involves an integration
over the $z^{(r)}$, which is generally not available in closed form. The
loglikelihood assuming that the latent $z^{(r)}$ are known is called the
complete-data loglikelihood. In our multitask setting, this has the form
\begin{align*}
\ell_{c}\left(\theta\right) = &\sum_{r = 1}^{p_{1}}\sum_{i =1 }^{n_{r}} \log p_{\theta}\left(y^{(r)}_{i}, \beta^{(r)}, w^{(r)}\right) \\
= &\sum_{r = 1}^{p_{1}}\sum_{i = 1}^{n_{r}} \log p_{\sigma^{2}}\left(y_{i}^{(r)} \mid \beta^{(r)}\right) + \log p_{\Lambda, \Psi}\left(\beta^{(r)} \mid w^{(r)}\right) + \log p_{\Phi}\left(w^{(r)}\right) \\
  = &\sum_{r = 1}^{p_{1}}\sum_{i = 1}^{n} \log \Gsn\left(y_{i}^{(r)} \mid x_{i}^{(r) T}\beta^{(r)}, \sigma^{2}\right) + \sum_{r = 1}^{p_{1}} n_{r}\left[\log \Gsn\left(\beta^{(r)} \mid S w^{(r)}, \Psi\right) + \log p_{\Phi}\left(w^{(r)}\right)\right],
\end{align*}
where, for simplicity, we have assumed that the $w^{(r)}$ are independent in the
prior across problems -- this covers most cases described in
\cite{zhang2005learning}, though variations can be implemented in the case that
they are not.

Now, since the $\beta^{(r)}$ and $w^{(r)}$ are unknown,
$\ell_{c}\left(\theta\right)$ cannot actually be evaluated. However, for
different choices of $q_{\gamma}$, it may be possible to evaluate
$\Esubarg{z^{(r)} \sim q_{\gamma}}{\ell_{c}\left(\theta\right)}$. Further,
for a fixed choice of $\theta$, it may be possible to choose $\gamma$ to
maximize this quantity, and vice versa. This is the variational EM strategy
adopted by \cite{zhang2005learning} -- initialize $\theta^{0}$, then choose
$\gamma^{1}$ to maximize
$\Esubarg{z^{(r)} \sim q_{\gamma}}{\ell_{c}\left(\theta^{0}\right)}$, then
find a $\theta^{1}$ to maximize
$\Esubarg{z^{(r)} \sim q_{\gamma^{1}}}{\ell_{c}\left(\theta\right)}$, and
so forth. The final posterior is approximated by $q_{\gamma^{t}}$ for some
large $t$.

For example, suppose our prior has the form choose
$w^{(r)} \sim \Mult\left(1, \pi\right)$, as in the clustered regressions
context. Then, a simple family $\left(q_{\gamma}\right)_{\gamma \in \Gamma}$ is
$q_{\gamma}\left(\beta^{(r)}, w^{(r)}\right) = \Gsn\left(\beta^{(r)} \mid m, V\right)\Mult\left(w^{(r)} \vert 1, \pi\right)$,
where the variational family is parameterized by
$\gamma = \left(m, V, \pi\right)$ and the latent data $\beta^{(r)}$ and $w^{(r)}$
are independent. Each term in the expected complete data
loglikehood can now be evaluated straightforwardly, for example, we can
write $\Esubarg{\beta^{r}, w^{(r)} \sim q_{\gamma}}{\log\Gsn\left(\beta^{(r)} \vert S w^{(r)}, \Psi\right)}$ as
\begin{align*}
  &-\frac{p}{2}\log 2\pi - \log \absarg{\Psi} -
  \frac{1}{2}\Esubarg{q_{\gamma}}{\beta^{(r) T}\Psi^{-1}\beta^{(r)}} +
  \Esubarg{q_{\gamma}}{s^{(r) T}\Psi^{-1}\beta^{(r)}} -
  \frac{1}{2}\Esubarg{q_{\gamma}}{w^{(r) T}S^{T}Sw^{(r)}} \\
  = &-\frac{p}{2}\log 2\pi - \log \absarg{\Psi} - \Psi^{-\frac{1}{2}}\left(V + mm^{T}\right)\Psi^{-\frac{1}{2}} + \pi^{T}S\Psi^{-1}m -\frac{1}{2}S \diag\left(\pi_{i}\right)S^{T}.
\end{align*}
Other terms can be calculated similarly. It is now a matter of calculus to find
the maximizing $\sigma^{2}, \Psi, S$, and $\Phi$ in the variational $M$-step,
and to find the maximizing $m, V$, and $\pi$ in the variational $E$-step.

\subsubsection{$\ell^{2, 1}$ Multitask learning}

An approach to multitask learning based in the $\ell^{2, 1}$ norm was derived in
\cite{argyriou2008convex}, who also extended the approach to the kernel setting.
We only describe a simplified approach here, focusing on the linear model case
with squared error loss. Denote the data $x_{i}^{(r)}$ and $y_{i}^{(r)}$ as
before. We can attempt to learn parameters $\beta^{(r)}$ for each task by
minimizing squared-error loss,
\begin{align}
\sum_{r = 1}^{p_{1}}\sum_{i = 1}^{n_{r}}\left(y_{i}^{r} - \beta^{(r) T}x_{i}\right)^{2},
\end{align}
but, as in the naive multiresponse linear regression described in Section
\ref{sec:r-reg}, this does not share information across problems. The approach
to pooling information across the tasks proposed by \cite{argyriou2008convex}
was to use the $\ell^{2, 1}$ matrix norm, defined by
$\|A\|_{1, 2} = \sum_{i = 1}^{n} \|a_{i \cdot}\|_{2}$, which can be thought of
as first taking a rowwise two-norm and applying the one-norm on the resulting
vector. Specifically, they consider the regularized problem

\begin{align}
\sum_{r = 1}^{p_{1}}\sum_{i = 1}^{n_{r}}\left(y_{i}^{r} - \beta^{(r) T}x_{i}\right)^{2} + \lambda \|B\|_{2, 1},
\end{align}
where $B \in \reals^{p_{2} \times p_{1}}$ has $r^{th}$ column $\beta^{(r)}$. The
$\ell^{2, 1}$-norm has the effect of sending entire rows of $B$ (features
measured in the $x_{i}$) two zero across tasks.

As is, this form is relatively well-known; for example, it is implemented in
\texttt{glmnet} via the "mgaussian" family \cite{friedman2009glmnet}.
\cite{argyriou2008convex} introduce several novelties, however. For example,
they consider fitting a model after first learning a lower-dimensional
representation of the $x_{i}$'s, given by $U^{T}x_{i}\in \reals^{K}$. That is,
they seek to optimize
\begin{align}
\sum_{r = 1}^{p_{1}}\sum_{i = 1}^{n_{r}}\left(y_{i}^{r} - \beta^{(r) T}U^{T}x_{i}\right)^{2} + \lambda \|B\|_{2, 1},
\end{align}
jointly over $U \in \reals^{p_{2} \times K}$ and
$B \in \reals^{K \times p_{1}}$. It is not clear how to perform this
optimization. However, \cite{argyriou2008convex} are able to demonstrate
that this problem has a convex reformulation. The new objective is
\begin{align}
\sum_{r = 1}^{p_{1}}\sum_{i = 1}^{n_{r}}\left(y_{i} - w_{r}^{T}x_{i}^{(r)}\right)^{2} + \lambda \sum_{r = 1}^{p_{1}}w_{r}^{T} D_{+} w^{r},
\end{align}
and the minimization is now done over $W \in \reals^{p_{2} \times p_{1}}$ and
$D_{+} \in S_{+}^{d}$. The relationship between the optimal parameters in the
two formulations is
\begin{align}
\left(\hat{W}, \hat{D}\right) &= \left(\hat{U}\hat{B}, \hat{U}\diag\left(\frac{\|\hat{b}_{i\cdot}\|_{2}}{\|\hat{B}\|_{2, 1}}\right)\hat{U}^{T}\right). \label{eq:reform_argryiou}
\end{align}

The proposed algorithm is to optimize the reformulated objective using an
alternating minimization, and then transform back to the original parameters
of interest via \ref{eq:reform_argryiou}.

\subsubsection{Graph-based Multitask Learning}

\cite{chen2010graph} describe an approach to multitask learning that
incorporates a priori knowledge on the relationship between different
regression problems. Specifically, they use a correlation network between
the responses of interest to induce a structured regularization on the
regression parameters across tasks.

In this setup, we will assume the features $x_{i}$ are shared across all
tasks, so that the data can be arranged in two matrices,
$Y \in \reals^{n \times p_{1}}$ and $X \in \reals^{n \times p_{2}}$, as in
the reduced-rank regression and C\&W procedure setup. We also assume a
correlation network between the $p_{2}$ tasks; this is denoted
$G = \left(V, E\right)$, where $V = \{1, \dots, p_{1}\}$. Each edge $e$ is
associated with a weight, $r\left(e\right)$, specifying the correlation between
the linked regression problems. This is assumed given, and is not learned by
this procedure.

The graph-fused lasso proposal is to obtain a coefficient matrix $B
\in \reals^{p_{2} \times p_{1}}$ whose columns $\beta^{(r)}$ are the
regression coefficients across each task, but which have been pooled
together, with the strength of the pooling depending on the
independently measured strength of the relationship between
tasks. This is formalized by defining $\hat{B}^{gf}$ as the solution
to the optimization,
\begin{align}
\minimize_{B \in \reals^{p_{2} \times p_{1}}} \frac{1}{2}\|Y -
  XB\|_{F}^{2} + \lambda \|B\|_{1} + \gamma \sum_{e \in E} \sum_{j =
    1}^{p_{2}} \absarg{r_{e}}\absarg{\beta_{j}^{(e^{+})} -
      \sign\left(r_{e}\right) \beta^{(e^{-})}_{j}}, \label{eq:gflasso_obj}
\end{align}
where $\|B\|_{1}$ is the sum of the absolute values of all entries of
$B$ and $e^{-}$ and $e^{+}$ denote the nodes at the ends of the edge
$e$. The last regularization term in the objective is called the
graph fused-lasso penalty, and it is the element that encourages
pooling of information across regression problems.

To write the graph fused-lasso penalty in matrix form, define a matrix
$\tilde{H} \in \reals^{p_{1} \times \absarg{E}}$, whose columns index
edges. For each column, place a 1 at the row corresponding to one
endpoint of the edge, and place a -1 at the edge corresponding to the
other. Then, $B\tilde{H}$ has $jl^{th}$ entry $\sum_{j = 1}^{p_{2}} \sum_{e :
  e^{+} = l}\beta_{j}^{(e^{+})} - \beta_{j}^{(e^{-})}$. In a similar
spirit, defining $H \in \reals^{p_{1} \times \absarg{E}}$ by, for each
edge (column of $H$), placing $\absarg{r_{e}}$ at the row
corresponding to one endpoint of that edge and
$-\sign\left(r_{e}\right)\absarg{r_{e}}$ gives
\begin{align}
\|BH\|_{1} &= \sum_{e \in E}\sum_{j = 1}^{p_{2}}
\absarg{r_{e}}\absarg{\beta_{j}^{(e^{+})} - \beta_{j}^{(e^{-})}},
\end{align}
and in particular we can write the objective \ref{eq:gflasso_obj} as
\begin{align}
  \frac{1}{2}\|Y - XB\|_{F}^{2} + \|BC\|_{1}, \label{eq:gflasso_reform}
\end{align}
where $C = \left(\lambda I_{p_{1}}, \gamma H\right)$.

At this point, the main question is how to recover the optimal
$\hat{B}^{\text{gf}}$. There are many ways to solve $\ell^{1}$
regularization problems -- for example, quadratic programming,
iterative soft-thresholding, coordinate descent, proximal smoothing,
and ADMM -- but \cite{chen2010graph} advocates a proximal smoothing
approach. That is, they apply gradient descent on a smooth surrogate
loss function; this is similar in spirit to optimizing a Huber loss
instead of an $\ell^{1}$-penalty. Rather than directly smoothing the
loss \ref{eq:gflasso_reform}, they first reformulate it as
\begin{align}
  \frac{1}{2}\|Y - XB\|_{F}^{2} + \max_{\|A\|_{\infty} \leq 1}
  \left<A, BC\right>,
\end{align}
using the duality between the $l^{1}$ and $\ell^{\infty}$ norms. It is
for this objective that a family of smooth surrogates is introduced,
\begin{align}
f_{\mu}\left(B\right) &:= \max_{\|A\|_{\infty} \leq 1}
\left[\left<A, BC\right> - \frac{\mu}{2}\|A\|_{F}^{2}\right],
\end{align}
and the new objective is to minimize
\begin{align}
\frac{1}{2}\|Y - XB\|_{F}^{2} + f_{\mu}\left(B\right).
\end{align}

When $\mu$ is 0, we recover the objective
\ref{eq:gflasso_reform}. When $\mu > 0$, the problem is smooth, and
its gradient can be found in closed form. Towards this, let
$g\left(X\right) = \frac{1}{2}\|X\|_{F}^{2}$ with domain restricted to
$\|X\|_{\infty} \leq 1$. Then $g$ has fenchel conjugate
$g^{\ast}\left(Y\right) = \max_{\|X\|_{\infty}\leq 1}\left<Y,
  X\right> - \frac{1}{2}\|X\|_{F}^{2}$. In particular,
\begin{align}
f_{\mu}\left(B\right) &=  \mu \max_{\|A\|_{\infty} \leq 1}
\left[\left<A, \frac{1}{\mu}BC\right> - \frac{1}{2}\|A\|_{F}^{2}\right] \\
&= \mu g^{\ast}\left(BC\right)
\end{align}
Using the fact that the derivative of a fenchel conjugate function is
given by the argmax of the maximization that defines it, we note
$\nabla g^{\ast}\left(Y\right) = P_{\ell^{\infty}}\left(Y\right)$ the
projection of $Y$ onto the $\ell^{\infty}$ ball\footnote{This is
  achieved by taking all entries larger than one and setting them to 1
  (and setting all entries smaller than -1 to -1).}. Together with the
  chain rule, this gives
\begin{align}
  \nabla f_{\mu}\left(B\right) &= P_{\ell^{\infty}}\left(\frac{1}{\mu}BC\right)C^{T},
\end{align}
and so the gradient of the objective \ref{eq:gflasso_reform} has the form
\begin{align}
 X^{T}\left(Y - XB\right) + P_{\ell^{\infty}}\left(\frac{1}{\mu}BC\right)C^{T},
\end{align}
which be input to any number of gradient-based routines;
\cite{chen2010graph} uses Nesterov's accelerated method, a version of
gradient descent with a momentum term.

\subsection{Matrix Factorization}

\bibliographystyle{unsrt}
\bibliography{microbiome_multitable}

\end{document}

