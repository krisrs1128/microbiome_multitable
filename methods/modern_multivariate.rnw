\documentclass{scrartcl}\usepackage{graphicx, color}
\usepackage{eulervm}
%% maxwidth is the original width if it is less than linewidth
%% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\definecolor{fgcolor}{rgb}{0.2, 0.2, 0.2}
\newcommand{\hlnumber}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlfunctioncall}[1]{\textcolor[rgb]{0.501960784313725,0,0.329411764705882}{\textbf{#1}}}%
\newcommand{\hlstring}[1]{\textcolor[rgb]{0.6,0.6,1}{#1}}%
\newcommand{\hlkeyword}[1]{\textcolor[rgb]{0,0,0}{\textbf{#1}}}%
\newcommand{\hlargument}[1]{\textcolor[rgb]{0.690196078431373,0.250980392156863,0.0196078431372549}{#1}}%
\newcommand{\hlcomment}[1]{\textcolor[rgb]{0.180392156862745,0.6,0.341176470588235}{#1}}%
\newcommand{\hlroxygencomment}[1]{\textcolor[rgb]{0.43921568627451,0.47843137254902,0.701960784313725}{#1}}%
\newcommand{\hlformalargs}[1]{\textcolor[rgb]{0.690196078431373,0.250980392156863,0.0196078431372549}{#1}}%
\newcommand{\hleqformalargs}[1]{\textcolor[rgb]{0.690196078431373,0.250980392156863,0.0196078431372549}{#1}}%
\newcommand{\hlassignement}[1]{\textcolor[rgb]{0,0,0}{\textbf{#1}}}%
\newcommand{\hlpackage}[1]{\textcolor[rgb]{0.588235294117647,0.709803921568627,0.145098039215686}{#1}}%
\newcommand{\hlslot}[1]{\textit{#1}}%
\newcommand{\hlsymbol}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlprompt}[1]{\textcolor[rgb]{0.2,0.2,0.2}{#1}}%

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}

\usepackage{alltt}
% ------------------------------------------------------------------------
% Packages
% ------------------------------------------------------------------------
\usepackage{amsmath,amssymb,amsfonts,graphicx,mathtools,setspace}
\usepackage[body={7in, 9in},left=1in,right=1in]{geometry}
\usepackage{url}
\usepackage[colorlinks]{hyperref}

% ------------------------------------------------------------------------
% Macros
% ------------------------------------------------------------------------
%~~~~~~~~~~~~~~~
% List shorthand
%~~~~~~~~~~~~~~~
\newcommand{\BIT}{\begin{itemize}}
\newcommand{\EIT}{\end{itemize}}
\newcommand{\BNUM}{\begin{enumerate}}
\newcommand{\ENUM}{\end{enumerate}}
%~~~~~~~~~~~~~~~
% Text with quads around it
%~~~~~~~~~~~~~~~
\newcommand{\qtext}[1]{\quad\text{#1}\quad}
%~~~~~~~~~~~~~~~
% Shorthand for math formatting
%~~~~~~~~~~~~~~~
\newcommand\mbb[1]{\mathbb{#1}}
\newcommand\mbf[1]{\mathbf{#1}}
\def\mc#1{\mathcal{#1}}
\def\mrm#1{\mathrm{#1}}
%~~~~~~~~~~~~~~~
% Common sets
%~~~~~~~~~~~~~~~
\def\reals{\mathbb{R}} % Real number symbol
\def\integers{\mathbb{Z}} % Integer symbol
\def\rationals{\mathbb{Q}} % Rational numbers
\def\naturals{\mathbb{N}} % Natural numbers
\def\complex{\mathbb{C}} % Complex numbers
%~~~~~~~~~~~~~~~
% Common functions
%~~~~~~~~~~~~~~~
\renewcommand{\exp}[1]{\operatorname{exp}\left(#1\right)} % Exponential
\def\indic#1{\mbb{I}\left({#1}\right)} % Indicator function
\providecommand{\argmax}{\mathop\mathrm{arg max}} % Defining math symbols
\providecommand{\argmin}{\mathop\mathrm{arg min}}
\providecommand{\arccos}{\mathop\mathrm{arccos}}
\providecommand{\dom}{\mathop\mathrm{dom}} % Domain
\providecommand{\range}{\mathop\mathrm{range}} % Range
\providecommand{\diag}{\mathop\mathrm{diag}}
\providecommand{\tr}{\mathop\mathrm{tr}}
\providecommand{\abs}{\mathop\mathrm{abs}}
\providecommand{\card}{\mathop\mathrm{card}}
\providecommand{\sign}{\mathop\mathrm{sign}}
\def\rank#1{\mathrm{rank}({#1})}
\def\supp#1{\mathrm{supp}({#1})}
%~~~~~~~~~~~~~~~
% Common probability symbols
%~~~~~~~~~~~~~~~
\def\E{\mathbb{E}} % Expectation symbol
\def\Earg#1{\E\left[{#1}\right]}
\def\Esubarg#1#2{\E_{#1}\left[{#2}\right]}
\def\P{\mathbb{P}} % Probability symbol
\def\Parg#1{\P\left({#1}\right)}
\def\Psubarg#1#2{\P_{#1}\left[{#2}\right]}
\def\Cov{\mrm{Cov}} % Covariance symbol
\def\Corr{\mrm{Corr}} % Covariance symbol
\def\Covarg#1{\Cov\left[{#1}\right]}
\def\Covsubarg#1#2{\Cov_{#1}\left[{#2}\right]}
\def\Corrsubarg#1#2{\Corr_{#1}\left[{#2}\right]}
\def\Var{\mrm{Var}}
\def\Vararg#1{\Var\left(#1\right)}
\def\Varsubarg#1#2{\Var_{#1}\left(#2\right)}
\newcommand{\family}{\mathcal{P}} % probability family
\newcommand{\eps}{\epsilon}
\def\absarg#1{\left|#1\right|}
\def\msarg#1{\left(#1\right)^{2}}
\def\logarg#1{\log\left(#1\right)}
%~~~~~~~~~~~~~~~
% Distributions
%~~~~~~~~~~~~~~~
\def\Gsn{\mathcal{N}}
\def\Ber{\textnormal{Ber}}
\def\Bin{\textnormal{Bin}}
\def\Unif{\textnormal{Unif}}
\def\Mult{\textnormal{Mult}}
\def\NegMult{\textnormal{NegMult}}
\def\Dir{\textnormal{Dir}}
\def\Bet{\textnormal{Beta}}
\def\Poi{\textnormal{Poi}}
\def\HypGeo{\textnormal{HypGeo}}
\def\GEM{\textnormal{GEM}}
\def\BP{\textnormal{BP}}
\def\DP{\textnormal{DP}}
\def\BeP{\textnormal{BeP}}
%~~~~~~~~~~~~~~~
% Theorem-like environments
%~~~~~~~~~~~~~~~
\newtheorem{definition}{Definition}
\newtheorem{example}{Example}
\newtheorem{theorem}{Theorem}

%-----------------------
% Probability sets
%-----------------------
\newcommand{\X}{\mathcal{X}}
\newcommand{\Y}{\mathcal{Y}}
%-----------------------
% vector notation
%-----------------------
\newcommand{\bx}{\mathbf{x}}
\newcommand{\by}{\mathbf{y}}
\newcommand{\bt}{\mathbf{t}}
\newcommand{\xbar}{\overline{x}}
\newcommand{\Xbar}{\overline{X}}
\newcommand{\tolaw}{\xrightarrow{\mathcal{L}}}
\newcommand{\toprob}{\xrightarrow{\mathbb{P}}}
\newcommand{\laweq}{\overset{\mathcal{L}}{=}}
\newcommand{\F}{\mathcal{F}}
\renewenvironment{knitrout}{\begin{singlespace}}{\end{singlespace}}
%-----------------------
% Math environments
%-----------------------
%\newcommand{\ba}{\begin{align}}
%\newcommand{\ea}{\end{align}}
%\newcommand{\ba}{\begin{align}}

<<echo = FALSE>>=
library("knitr")
library("ggplot2")
library("grid")
opts_chunk$set(cache = TRUE, fig.width = 4, fig.height = 4, fig.align = "center")

scale_colour_discrete <- function(...)
  scale_colour_brewer(..., palette="Set2")
small_theme <- function(base_size = 7, base_family = "Helvetica") {
  theme_bw(base_size = base_size, base_family = base_family) %+replace%
  theme(axis.title = element_text(size = rel(0.8)),
        legend.title = element_text(size = rel(0.8)),
        legend.text = element_text(size = rel(0.8)),
        plot.title = element_text(size = rel(1.1)),
        plot.margin = unit(c(0.1, .1, .1, .1), "lines")
        )
}

#read_chunk()
@

\linespread{1.5}

\begin{document}

\subsection{Modern multivariate methods}

Compared to classical approaches, modern multivariate methods
are typically designed for more the high-dimensional, heterogeneous
settings. The two methods reviewed in this section are examples of
this trend: partial least squares is well-suited for
high-dimensional spectra (which explains its popularity in
chemometrics), while canonical correspondence analysis was
designed for joint analysis of the heterogeneous continuous / count
data necessary to answer questions in ecology. Unlike traditional
statistical methods, neither approach is explicitly model-based, and
both are iterative, requiring more extensive computation than earlier
techniques.

\subsubsection{Partial Least Squares (PLS)}

PLS is a procedure for sequentially deriving a set of mutually
orthogonal features $\left(z_{k}\right)_{k = 1}^{K}$ which summarize
the relationship between two tables, $X^{(1)}$ and $X^{(2)}$
\cite{wold1985partial}. To obtain the first PLS direction, $z_{1}$,
compute the first left singular vector $u_{1}$ of the cross-covariance
matrix between the two tables, $\hat{\Sigma}_{12} =
\frac{1}{n}X^{(1)T}X^{(2)}$. Then, for each of the $p_{2}$ columns of
$X^{(2)}$, compute the univariate (i.e., partial) regression coefficient
$\hat{\varphi}_{j}$ from the model $u_{1i} = \alpha_{0j} +
\varphi_{j}X^{(2)}_{ij}$, for $i = 1, \dots, j_{1}$. The first PLS
direction is defined as $z_{1} = X^{(2)}\hat{\varphi}_{1}$. To
generate subsequent directions, orthogonalize both $X^{(1)}$ and
$X^{(2)}$ with respect to the current directions, and repeat the process.

This procedure is appealing because, like PCA, it reduces a potentially
high-dimensional matrix $X^{(2)}$ with many correlated columns into a
smaller set of orthogonal directions. Moreover, it achieves this
reduction in a way that accounts for correlation with columns in
x$X^{(1)}$: columns of $X^{(2)}$ that are uncorrelated with $X^{(1)}$
will have no contribution to the PLS directions, even if they account
for a large proportion of variation in $X^{(2)}$.

We have stated the procedure in the form it was originally
proposed; its algorithmic nature makes it difficult to understand
geometrically or probabilsitically. Subsequently, several authors have
attempted to provide statistical interpretations for the PLS
directions. \cite{frank1993statistical} \cite{stone1990continuum} studied
the case where $p_{1} = 1$, so $X^{(1)}$ is a single column
vector. They further suppose the rows of $X^{(1)}$ and $X^{(2)}$ are
drawn iid from distributions $\P^{(1)}$ and $\P^{(2)}$. They found
that the $k^{th}$ PLS direction $z_{k}$ is the $z$ that solves the
optimization
\begin{align}
\max_{z} \medspace & \Corrsubarg{\P^{(1)} \times \P^{(2)}}{x_{i}^{(2) T}z_{k},
x_{i}^{(1)}}\Varsubarg{\P^{(2)}}{z^{T}x_i^{(2)}} \\
\text{such that }&z^{T}X^{(1) T}X^{(1)}z_{j} = 0 \text{ for all }j
\leq k - 1 \text{ and }
\|z\| = 1.
\end{align}
If the covariance term is ommitted, the optimization is identical to
the maximum variance problem that gives the principal component
directions based on $X^{(2)}$. This formulation makes precise the idea
that PLS is a version of principal components that accounts for
correlation with $X^{(1)}$.

An alternative interptetation, due to
\cite{gustafsson2001probabilistic}, is that PLS is the solution to a
particular latent variable model. Suppose $\xi_{i} =
\left(\xi_{i}^{shared}, \xi_{i}^{(2)}\right)$ are drawn iid from a $K_{1} +
K_{2} = K$ dimensional spherical normal. The observed tables $X^{(1)}$
and $X^{(2)}$ have rows drawn iid from
\begin{align}
x_{i}^{(1)} \mid \xi_{i} \sim \Gsn\left(W^{(1)}\xi_{i}^{(shared)} +
  \mu^{(1)}, \sigma^{2}I_{p_{1}}\right) \\
x_{i}^{(2)} \mid \xi_{i} \sim \Gsn\left(W^{(2)}\xi_{i}^{(shared)} +
  \mu^{(2)}, \sigma^{2}I_{p_{2}}\right).
\end{align}

That is, each table is the sum of two components, one that is a
table-specific linear combination is a shared latent variable, and
another that is an arbitrary linear combination of a table-specific
latent variable. The shared feature $\xi^{shared}$ is the object of
interest, and is what PLS implictly estimates.

\subsubsection{Canonical Correspondence Analysis (CCA)}

CCA is a two table method designed to answer questions in ecology
involving joint analysis of count and continuous data. For example
$X^{(1)}$ might be a table whose rows are ecological sites, and whose
columns are the counts of different species at those sites, while
$X^{(2)}$ might describe environmental characteristics at those sites
(e.g., historical rainfall and temperature measurements). A scientific
goal might be to identify those species that are more abundant in
sites with more rainfall or higher temperature. If these environmental
variables were uncorrelated, it would be enough to fit a separate
regression to each; this however is rarely the case, warranting the
need for CCA.

CCA produces low-dimensional representations for both the rows and
columns of $X^{(1)}$ (the sites and species), as well as the latent
subspaces onto which these representations are
projected. Algorithmically, CCA proceeds first constructs the
following matrices, where $1_{r}$ denotes a column vector of $r$ ones,
\begin{enumerate}
  \item An overall frequency matrix,
    \begin{align}
      F =
    \frac{1}{n_{\cdot\cdot}^{(1)}} X^{(1)},
  \end{align}
  where $n_{\cdot\cdot}^{(1)}$ is the sum of all counts in matrix
  $X^{(1)}$.
\item A diagonal matrix of row (site) proportions,
  \begin{align}
    D_{r} &= diag\left(F 1_{p_{1}}\right) \in \reals^{n}.
  \end{align}
\item A diagonal matrix of column (species) proportions,
  \begin{align}
    D_{c} &= diag\left(F^{T}1_{n}\right) \in \reals^{p}.
  \end{align}
\item A projection onto the columns of the environmental matrix
  $X^{(2)}$, reweighting sites according to their species counts,
  \begin{align}
    P_{X^{(2)}} &= D_{r}^{\frac{1}{2}}X^{(2)}\left(X^{(2)
      T}D_{r}X^{(2)}\right)^{-1}X^{(2) T}D_{r}^{\frac{1}{2}} \in
  \reals^{n \times n}.
\end{align}
\end{enumerate}

With these variables set up, compute an SVD,
\begin{align}
D_{r}^{-\frac{1}{2}}\left(F - F
  1_{p}1_{p}^{T}F\right)D_{c}^{-\frac{1}{2}}P_{X} = UDV^{T},
\end{align}
and define row and column scores $Z$ and $Q$ by
\begin{align}
  Z &= D_{r}^{-\frac{1}{2}} UD \\
  Q &= D_{c}^{-\frac{1}{2}}V^{T}D.
\end{align}

There are several ways to interpret this procedure. The original
proposal described CCA as a solution to a fixed-point iteration; they
also demonstrated that this was the approximate MLE of a probability
model \cite{ter1986canonical}. Later, \cite{greenacre1987geometric,
  greenacre1984theory}, provided a geometric view, and
\cite{zhu2005constrained} gave an exact probabilistic
interpretation. Here, we describe the fixed-point iteration
motivation, since it was originating idea for the method, and also the
exact probabilistic interpretation, to continue the theme of
describing implicit probability models behind different algorithmic
dimensionality reduction procedures.

The intuition for the reciprocal averaging procedure is simple: the
scores for different sites should be a weighted average of the species
scores, with larger weights for the species that are more common at
those sites. That is,
\begin{align}
  z_{i} \propto \frac{1}{f_{i\cdot}}\sum_{j = 1}^{p_{1}}f_{ij}q_{ij} \\
  q_{j} \propto \frac{1}{f_{\cdot j}} \sum_{i = 1}^{n} f_{ij}z_{ij},
\end{align}
or, in matrix form,
\begin{align}
Z \propto \diag\left(F 1_{p}\right)^{-1} F Q^{T} \\
Q \propto \diag\left(F^{T} 1_{n}\right)^{-1} P.
\end{align}
This formulation suggests a method to obtain $Z$ and $Q$ --
arbitrarily initialize one and iterate until convergence.

As is, this is not quite the setup that leads to CCA\footnote{It in
  fact gives the solution to the Correspondence Analysis problem (the
  similarity is the reason for the name \textit{Canonical}
  Correspondence Analysis).} -- it doesn't even use information in the
environmental variables table $X^{(2)}$. To recover CCA, a projection
step needs to be inserted before the calculation of row scores,
\begin{enumerate}
\item Arbitrarily initialize $Z$.
\begin{enumerate}
\item Solve $Q^{\prime} \propto \diag\left(F^{T}1_{n}\right)^{-1}F^{T}Z$.
\item Project $Q = P_{X^{(2)}}Q^{\prime}$.
\item Solve $Z \propto \diag\left(Z 1_{p}\right)^{-1} F Q^{T}$.
\end{enumerate}
\end{enumerate}

The iteration now converges to the previously described CCA solution.

A second, less widely appreciated, interpretation, is due to
\cite{zhu2005constrained}. Suppose for now that we are only interested
in a one-dimensional score for rows and columns. Suppose $\alpha$ is a
latent environmental gradient, for example, between warm-dry and
cold-wet sites. For each of the $p_{1}$ species, define a normal
density over the environmental variables,
$f_{j}\left(x_{i}^{(2)}\right) = \Gsn_{p_{(2)}}\left(\mu_{j},
  \Sigma_{j}\right)$. The mode of this density represents the
preferred environmental conditions for species $j$.  Next, project
these densities onto the environmental gradient, giving a univariate
$f_{j}^{\alpha}\left(z_{i}\right) = \Gsn\left(\alpha^{T}\mu_{j},
  \alpha^{T} \Sigma_{j}\alpha\right)$ for each species. The $z_{i}$'s
represent the scores for species $i$ along the environmental gradient
$\alpha$.

The generative model views each species-site pair one at a time. For
each pair involving site $i$ and species $j$ we draw a score according
to $f_{j}^{\alpha}\left(z_{i}\right)$. So, for each site $i$, we draw
a species according to a $p_{1}$-class LDA model.

To use this idea to estimate scores, we need to estimate the
environmental gradient  $\alpha$ (which is also of interest in its own
right). This is done by supposing equal covariances across species,
$\Sigma_{j} = \Sigma$ for all $j$, and finding the $\alpha$ that
maximizes the between vs. total variance across species,
\begin{align}
  \frac{\alpha^{T} \Sigma_{B} \alpha}{\alpha^{T} \Sigma \alpha},
\end{align}
where
\begin{align}
  \Sigma_{B} = \sum_{j = 1}^{p_{1}} f_{\cdot j}\left(\mu_{j} -
    \bar{\mu}\right)\left(\mu_{j} - \bar{\mu}\right)^{T}.
\end{align}

Estimating $\hat{\alpha}$ in this way and writing $z_{i} =
\hat{\alpha}^{T}x_{i}$ gives the original site scores from CCA.

\bibliographystyle{unsrt}
\bibliography{microbiome_multitable}

\end{document}
