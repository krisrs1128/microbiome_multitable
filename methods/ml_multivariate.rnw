\documentclass{scrartcl}\usepackage{graphicx, color}
\usepackage{eulervm}
%% maxwidth is the original width if it is less than linewidth
%% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\definecolor{fgcolor}{rgb}{0.2, 0.2, 0.2}
\newcommand{\hlnumber}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlfunctioncall}[1]{\textcolor[rgb]{0.501960784313725,0,0.329411764705882}{\textbf{#1}}}%
\newcommand{\hlstring}[1]{\textcolor[rgb]{0.6,0.6,1}{#1}}%
\newcommand{\hlkeyword}[1]{\textcolor[rgb]{0,0,0}{\textbf{#1}}}%
\newcommand{\hlargument}[1]{\textcolor[rgb]{0.690196078431373,0.250980392156863,0.0196078431372549}{#1}}%
\newcommand{\hlcomment}[1]{\textcolor[rgb]{0.180392156862745,0.6,0.341176470588235}{#1}}%
\newcommand{\hlroxygencomment}[1]{\textcolor[rgb]{0.43921568627451,0.47843137254902,0.701960784313725}{#1}}%
\newcommand{\hlformalargs}[1]{\textcolor[rgb]{0.690196078431373,0.250980392156863,0.0196078431372549}{#1}}%
\newcommand{\hleqformalargs}[1]{\textcolor[rgb]{0.690196078431373,0.250980392156863,0.0196078431372549}{#1}}%
\newcommand{\hlassignement}[1]{\textcolor[rgb]{0,0,0}{\textbf{#1}}}%
\newcommand{\hlpackage}[1]{\textcolor[rgb]{0.588235294117647,0.709803921568627,0.145098039215686}{#1}}%
\newcommand{\hlslot}[1]{\textit{#1}}%
\newcommand{\hlsymbol}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlprompt}[1]{\textcolor[rgb]{0.2,0.2,0.2}{#1}}%

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}

\usepackage{alltt}
% ------------------------------------------------------------------------
% Packages
% ------------------------------------------------------------------------
\usepackage{amsmath,amssymb,amsfonts,graphicx,mathtools,setspace}
\usepackage[body={7in, 9in},left=1in,right=1in]{geometry}
\usepackage{url}
\usepackage[colorlinks]{hyperref}

% ------------------------------------------------------------------------
% Macros
% ------------------------------------------------------------------------
%~~~~~~~~~~~~~~~
% List shorthand
%~~~~~~~~~~~~~~~
\newcommand{\BIT}{\begin{itemize}}
\newcommand{\EIT}{\end{itemize}}
\newcommand{\BNUM}{\begin{enumerate}}
\newcommand{\ENUM}{\end{enumerate}}
%~~~~~~~~~~~~~~~
% Text with quads around it
%~~~~~~~~~~~~~~~
\newcommand{\qtext}[1]{\quad\text{#1}\quad}
%~~~~~~~~~~~~~~~
% Shorthand for math formatting
%~~~~~~~~~~~~~~~
\newcommand\mbb[1]{\mathbb{#1}}
\newcommand\mbf[1]{\mathbf{#1}}
\def\mc#1{\mathcal{#1}}
\def\mrm#1{\mathrm{#1}}
%~~~~~~~~~~~~~~~
% Common sets
%~~~~~~~~~~~~~~~
\def\reals{\mathbb{R}} % Real number symbol
\def\integers{\mathbb{Z}} % Integer symbol
\def\rationals{\mathbb{Q}} % Rational numbers
\def\naturals{\mathbb{N}} % Natural numbers
\def\complex{\mathbb{C}} % Complex numbers
%~~~~~~~~~~~~~~~
% Common functions
%~~~~~~~~~~~~~~~
\renewcommand{\exp}[1]{\operatorname{exp}\left(#1\right)} % Exponential
\def\indic#1{\mbb{I}\left({#1}\right)} % Indicator function
\providecommand{\argmax}{\mathop\mathrm{arg max}} % Defining math symbols
\providecommand{\argmin}{\mathop\mathrm{arg min}}
\providecommand{\maximize}{\mathop\mathrm{maximize\quad}} % Defining math symbols
\providecommand{\minimize}{\mathop\mathrm{minimize\quad}} % Defining math symbols
\providecommand{\arccos}{\mathop\mathrm{arccos}}
\providecommand{\dom}{\mathop\mathrm{dom}} % Domain
\providecommand{\range}{\mathop\mathrm{range}} % Range
\providecommand{\diag}{\mathop\mathrm{diag}}
\providecommand{\tr}{\mathop\mathrm{tr}}
\providecommand{\abs}{\mathop\mathrm{abs}}
\providecommand{\card}{\mathop\mathrm{card}}
\providecommand{\sign}{\mathop\mathrm{sign}}
\def\rank#1{\mathrm{rank}({#1})}
\def\supp#1{\mathrm{supp}({#1})}
%~~~~~~~~~~~~~~~
% Common probability symbols
%~~~~~~~~~~~~~~~
\def\E{\mathbb{E}} % Expectation symbol
\def\Earg#1{\E\left[{#1}\right]}
\def\Esubarg#1#2{\E_{#1}\left[{#2}\right]}
\def\P{\mathbb{P}} % Probability symbol
\def\Parg#1{\P\left({#1}\right)}
\def\Psubarg#1#2{\P_{#1}\left[{#2}\right]}
\def\Cov{\mrm{Cov}} % Covariance symbol
\def\Covarg#1{\Cov\left[{#1}\right]}
\def\Covsubarg#1#2{\Cov_{#1}\left[{#2}\right]}
\def\Var{\mrm{Var}}
\def\Vararg#1{\Var\left(#1\right)}
\def\Varsubarg#1#2{\Var_{#1}\left(#2\right)}
\newcommand{\family}{\mathcal{P}} % probability family
\newcommand{\eps}{\epsilon}
\def\absarg#1{\left|#1\right|}
\def\msarg#1{\left(#1\right)^{2}}
\def\logarg#1{\log\left(#1\right)}
%~~~~~~~~~~~~~~~
% Distributions
%~~~~~~~~~~~~~~~
\def\Gsn{\mathcal{N}}
\def\Ber{\textnormal{Ber}}
\def\Bin{\textnormal{Bin}}
\def\Unif{\textnormal{Unif}}
\def\Mult{\textnormal{Mult}}
\def\NegMult{\textnormal{NegMult}}
\def\Dir{\textnormal{Dir}}
\def\Bet{\textnormal{Beta}}
\def\Poi{\textnormal{Poi}}
\def\HypGeo{\textnormal{HypGeo}}
\def\GEM{\textnormal{GEM}}
\def\BP{\textnormal{BP}}
\def\DP{\textnormal{DP}}
\def\BeP{\textnormal{BeP}}
%~~~~~~~~~~~~~~~
% Theorem-like environments
%~~~~~~~~~~~~~~~
\newtheorem{definition}{Definition}
\newtheorem{example}{Example}
\newtheorem{theorem}{Theorem}

%-----------------------
% Probability sets
%-----------------------
\newcommand{\X}{\mathcal{X}}
\newcommand{\Y}{\mathcal{Y}}
%-----------------------
% vector notation
%-----------------------
\newcommand{\bx}{\mathbf{x}}
\newcommand{\by}{\mathbf{y}}
\newcommand{\bt}{\mathbf{t}}
\newcommand{\xbar}{\overline{x}}
\newcommand{\Xbar}{\overline{X}}
\newcommand{\tolaw}{\xrightarrow{\mathcal{L}}}
\newcommand{\toprob}{\xrightarrow{\mathbb{P}}}
\newcommand{\laweq}{\overset{\mathcal{L}}{=}}
\newcommand{\F}{\mathcal{F}}
\renewenvironment{knitrout}{\begin{singlespace}}{\end{singlespace}}
%-----------------------
% Math environments
%-----------------------
%\newcommand{\ba}{\begin{align}}
%\newcommand{\ea}{\end{align}}
%\newcommand{\ba}{\begin{align}}

<<echo = FALSE>>=
library("knitr")
library("ggplot2")
library("grid")
opts_chunk$set(cache = TRUE, fig.width = 4, fig.height = 4, fig.align = "center")

scale_colour_discrete <- function(...)
  scale_colour_brewer(..., palette="Set2")
small_theme <- function(base_size = 7, base_family = "Helvetica") {
  theme_bw(base_size = base_size, base_family = base_family) %+replace%
  theme(axis.title = element_text(size = rel(0.8)),
        legend.title = element_text(size = rel(0.8)),
        legend.text = element_text(size = rel(0.8)),
        plot.title = element_text(size = rel(1.1)),
        plot.margin = unit(c(0.1, .1, .1, .1), "lines")
        )
}

#read_chunk()
@

\linespread{1.5}

\begin{document}

\subsection{Methods from Machine Learning}

Several different literatures within machine learning have each
developed approaches to dimensionality reduction across multiple
tables, each relying on the tools studied in that literature. In this
section, we will study some methods that have arisen in the kernel
learning, spectral clustering, regularized modeling, and probabilistic
inference communities. A common theme across these methods is that
they provide the scientist flexiblity in specifying the form of the
data model; these literatures emphasize tools that can be adapted to
the structure of a specific dataset, rather than algorithms that
should be applied generally. However, as these methods have typically
not been applied to ecological or microbiome studies, it is not yet
clear what their overall utility will be.

\subsubsection{Kernel Canonical Correlation Analysis (KCCA)}

KCCA is a variant of CCA designed to be sensitive to nonlinear
associations across tables \cite{akaho2006kernel, bach2003kernel}. It
does this by implicitly lifting the original
data to a richer feature space, with the hope that nonlinear
associations in the original space become linear associations in the
richer space. Informaly, KCCA is the algorithm that emerges after
applying the kernel trick to CCA.

More formally, let $\varphi^{\left(l\right)}: \reals^{p_{l}}\mapsto
H^{(l)}$ be a mapping from the space associated with table $l$ to a
richer Hilbert space. For example, $\varphi^{l}$ might map vectors
into an expansion of all polynomial products of coordinates, up to
some fixed kernel; this is mapping is called the polynomial
kernel. As in CCA, let $\P^{(1)}$ and $\P^{(2)}$ denote the sampling
distributions associated with the two tables; let $x_{i}^{(1)}$ and
$x_{i}^{(2)}$ denote generic draws from these tables.

In the same way that the first CCA direction maximizes the covariance
between linear combinations $c_{1}^{(1) T}x_{i}^{(1)}$ and $c_{1}^{(2)
  T}x_{i}^{(2)}$ (subject to a variance constraint), KCCA maximizes
the correlation between linear functionals, $z_{i1}^{(1)} =
\left<c_{1}^{(1)}, x_{i}^{(1)}\right>$ and $z_{i1}^{(2)} =
\left<c_{1}^{(2)}, x_{i}^{(2)}\right>$.
\begin{align}
  \argmax_{c_{1}^{(1)} \in H^{(1)}, c_{1}^{(2)} \in H^{(2)}}
  &\Covsubarg{\P^{(1)} \times \P^{(2)}}{z_{i1}^{(1)}, z_{i1}^{(2)}} \\
\text{subject to }\Varsubarg{P^{(1)}}{z_{i1}^{(1)}} &=
\Varsubarg{P^{(2)}}{z_{i1}^{(2)}} = 1. \label{eq:optim_kcca}
\end{align}

As is, the problem is not well-posed, and it is necessarily to
regularize. The regularized Lagrangian associated with the
optimization \ref{eq:optim_kcca} is
\begin{align*}
&\Covsubarg{\P^{(1)} \times \P^{(2)}}{z_{i1}^{(1)}, z_{i1}^{(2)}} - \\
&\frac{\rho^{(1)}}{2}\Varsubarg{P^{(1)}}{z_{i1}^{(1)}} -
\frac{\rho^{(2)}}{2}\Varsubarg{P^{(2)}}{z_{i1}^{(2)}} \\+
&\frac{\lambda^{(1)}}{2}Pen\left(c_{1}^{(1)}\right) +
\frac{\lambda^{(2)}}{2}Pen\left(c_{1}^{(2)}\right) \label{eq:lagrangian_kcca}
\end{align*}

Now, note that the optimal $c_{1}^{(1)}$ and $c_{1}^{(2)}$ must lie in
the spans of $\left(\varphi^{(1)}\left(x_{i}\right)\right)_{i =1
}^{n}$ and $\left(\varphi^{(2)}\left(x_{i}\right)\right)_{i =1 }^{n}$, respectively,
since directions orthogonal to this subspace cannot improve the
correlation in the objective. Therefore, we can write
\begin{align}
c_{1}^{(1)} &= \Phi^{(1)} \alpha^{(1)} \\
c_{1}^{(2)} &= \Phi^{(2)} \alpha^{(2)},
\end{align}
where $\Phi^{(l)} \in \reals^{n \times
  \text{dim}\left(H^{(l)}\right)}$ has $i^{th}$ row
$\varphi^{(l)}\left(x_{i}\right)$.

Substituting this into the Lagrangian \ref{eq:lagrangian_kcca}, it
becomes clear that only the cross-products $\Phi^{(1) T}\Phi^{(1)}$,
and $\Phi^{(2) T}\Phi^{(2)}$ appear. Since these inner products can be
written as kernel matrices $K^{(l) (l)} \in \reals^{n \times n}$ with
entries $\varphi^{(l)
  T}\left(x^{(1)}_{i}\right)\varphi^{\left(l\right)}\left(x^{(2)}_{j}\right)$,
we have that the optimization can be fully kernelized.  It can then be
shown that the optimal $\alpha^{(1)}$ and $\alpha^{(2)}$ are the
solutions to the generalized eigenvalue problem,
\begin{align}
  \begin{pmatrix}
    0 & K^{(1)}K^{(2)} \\
    K^{(2)}K^{(1)} & 0
  \end{pmatrix}
  \begin{pmatrix}
    \alpha^{(1)} \\
    \alpha^{(2)}
  \end{pmatrix}
&= \rho
\begin{pmatrix}
  \left(K^{(1)} + \lambda^{(1)}I_{p_{1}}\right)^{2} & 0 \\
  0 & \left(K^{(2)} + \lambda^{(2)}I_{p_{2}}\right)^{2}
\end{pmatrix}
\begin{pmatrix}
  \alpha^{(1)} \\
  \alpha^{(2)}
\end{pmatrix}.
\end{align}

The contribution of \cite{yamanishi2003extraction} was to extend this
procedure to more than 2 tables. The approach is similar to the idea
of average pairwise covariance, discussed in the section about
CCA. Formally, if we let $z_{i1}^{(l)} = \left<c_{1}^{(l)|},
  x_{i}^{(l)}\right>$ and try to optimize

\begin{align*}
&\sum_{l, l^{\prime} = 1}^{L}\Covsubarg{\P^{(l)} \times \P^{(l^{\prime})}}{z_{i1}^{(l)}, z_{i1}^{(l^{\prime})}} -
\sum_{l = 1}^{L}\frac{\rho^{(l)}}{2}\Varsubarg{P^{(l)}}{z_{i1}^{(l)}} -
 \sum_{l = 1}^{L} \frac{\lambda^{(1)}}{2}Pen\left(c_{1}^{(l)}\right) \label{eq:lagrangian_mkcca}
\end{align*}

Replacing the inner products $\Phi^{(l)}\Phi^{(l)}$ as before leads to
an analogous generalized eigenvalue problem,
\begin{align}
\begin{pmatrix}
0 & \dots & K_{1}K_{L} \\
\vdots & \ddots & \vdots \\
K_{L}K_{1} & \dots  & 0
\end{pmatrix}
\begin{pmatrix}
  \alpha_{1} \\
  \vdots \\
  \alpha_{L}
\end{pmatrix} &=
\rho \begin{pmatrix}
  \left(K_{1} + \lambda_{1}I_{p_{1}}\right)^{2} & \dots & 0 \\
  \vdots & \ddots & \vdots \\
  0 & \dots & \left(K_{L} + \lambda_{L}I_{p_{L}}\right)^{2}
\end{pmatrix}
\begin{pmatrix}
  \alpha_{1} \\
  \vdots \\
  \alpha_{L}
\end{pmatrix}
\end{align}
whose solution is called ``multiple kernel canonical correlation
analysis'' by \cite{yamanishi2003extraction}.

A geometric interpretation of Kernel CCA was described in
\cite{kuss2003geometry}. The core idea is to translate the Euclidean
picture associated with CCA to the more general RKHS setting. Often,
however, KCCA results can be difficult to study, because the only
output are the scores relating samples (eigenvectors are never
computed in the Hilbert spaces $H^{(l)}$, so it's impossible to make a
biplot). However, the KCCA scores can be interpreted using
supplementary features, as in any other dimensionality reduction
method.

An complementary formulation of Kernel CCA was presented in
\cite{lanckriet2004statistical} (todo: maybe describe this approach
too?).

\subsubsection{Penalized Matrix Decomposition}

In high-dimensional settings, sparsity is a desirable property, both
for interpretability and statistical stability. For example, in the
regression setting, an $\ell^{1}$-penalty can be used to ensure that
only a few coefficients are nonzero. A regression model using only a
few features is easier to understand than one involving a linear
combination of all possible features. Further, regularized models
typically outperform their non-regularized counterparts; in fact, it
is impossible to fit a non-regularized linear regression when the
number of features is greater than the number of samples.

The Penalized Matrix Decomposition (PMD) is a general approach for
adapting the reguarlization machinery developed around regression to
the multivariate analysis setting
\cite{witten2009penalized}. Furthermore, a CCA and MultiCCA instance
of the PMD has been well-studied \cite{witten2009penalized,
  witten2013package}.

The general setup is as follows. Suppose we only want a
one-dimensional representation of samples, and are basing analysis on
a single matrix $X \in \reals^{n \times p}$. Recall that the first
$k$-eigenvectors recovered by PCA span a subspace that minimizes the
$L^{2}$ distance from the original data (rows of $X$, viewed as points
in $\reals^{p}$) to their projection onto that subspace. In
particular, when $k = 1$, the associated PCA coordinates $u \in
\reals^{n}$ and eigenvector $v$ can be \textit{defined} as the optimal
values in the problem
\begin{align}
  \minimize_{u \in \reals^{n}, v \in \reals^{p}, d \in \reals} &\|X - duv^{T}\|_{2}^{2} \\
  \text{subject to } &\|u\|_{2}^{2} = \|v\|_{2}^{2} = 1.
\end{align}

The PMD generalizes this formulation of rank-one PCA to enforce
structure (e.g., sparsity) on $u$ and $v$. The PMD solutions $u$ and
$v$ are defined to be the optimizers of
\begin{align}
\label{eq:pmd_opt} \minimize_{u \in \reals^{n}, v \in \reals^{p}, d
  \in \reals} &\|X - duv^{T}\|_{2}^{2} \\
  \text{subject to } &\|u\|_{2}^{2} = \|v\|_{2}^{2} = 1 \\
  & \text{Pen}_{1}\left(u\right) \leq mu_{1} \\
  & \text{Pen}_{2}\left(v\right) \leq \mu_{2}.
\end{align}

This only defines one dimensional approximations. To obtain a sequence
of scores / eigenvectors $\left(u_{k}\right)_{k = 1}^{K}$ and
$\left(v_{k}\right)_{k = 1}^{K}$, define $u_{k}$ and $v_{k}$ as the
optimizers of the problem \ref{eq:pmd_opt} on the residual $X^{k}$
obtained by subtracting out the previous approximation: $X^{k} := X^{k
- 1} - d_{k - 1}u_{k - 1}v_{k - 1}^{T}$ where $d_{k} = u_{k}^{T}
X^{k}v_{k}$ and $X^{1} = X$.

(todo: geometric interpretation)
(todo: generalized least squares / probabilistic interpretation
\cite{allen2014generalized})

This view can be specialized to develop regularized versions of a
number of multivariate analysis problems; here consider applications
to the CCA and Multiple CCA\footnote{CCA with $L$ tables, as in
  multiple kernel CCA above.} problems.

Using the fact that $\|A\|_{F}^{2} = \tr\left(A^{T}A\right)$ along
with the linearity and the cyclic property of the trace, the objective
in \ref{eq:pmd_opt} can be rewritten (using $\equiv$ to mean equality
up to terms constant in $u$ and $v$),
\begin{align}
  \|X - duv^{T}\|_{F}^{2} &\equiv \tr\left(\left(X -
      duv^{T}\right)^{T}\left(X - duv^{T}\right)\right) \\
  &\equiv -2d\tr\left(X^{T}uv^{T}\right) + d^{2}tr\left(uv^{T}vu^{T}\right) \\
  &\equiv -2d v^{T}X^{T}u + d^{2}
\end{align}
where for the last equivalence we used that $v^{T}v = u^{T}u = 1$.

From this expression, and by partially minimizing out $d =
v^{T}X^{T}u$, we see that the PMD solutions $u$ and $v$ in
\ref{eq_pmd_opt} can be found as the optimizers of
\begin{align}
\label{eq:pmd_reform}  \maximize_{u \in \reals^{n}, v \in \reals^{p}} &u^{T}X^{T}v \\
  \text{subject to } &\|u\|_{2}^{2} = \|v\|_{2}^{2} \\
  &\text{Pen}_{1}\left(u\right) \leq \mu_{1} \\
  &\text{Pen}_{2}\left(v\right) \leq \mu_{2}
\end{align}

Notice that, as long as the penalties are convex in $u$ and $v$, the
optimization is biconvex, so a local maximum can be found by
alternately maximizing over $u$ and $v$.

It is not in general obvious how to choose the regularization
parameters $\mu_{1}$ and $\mu_{2}$. For example, if the penalties are
$\ell^{1}$-norms, then these parameters would control the overall
sparsity of the underlying factors. The proposal in
\cite{witten2009penalized} was to apply cross-validation on the
reconstruction errors after holding out random entries in $X$.

From this form, we can derive a PMD, sparsity-inducing version of
CCA. Recall the maximal-covariance interpretation of CCA,
\begin{align}
  \maximize_{c_{1}^{(1)} \in \reals^{p_{1}}, c_{1}^{(2)} \in
    \reals^{p_{2}}} &\Covsubarg{\P^{(1)}, \P^{(2)}}{c_{1}^{(1)
      T}x_{i}^{(1)},
    c_{1}^{(2) T} x_{i}^{(2)}}, \\
  &\Varsubarg{\P^{(1)}}{x_{i}^{(1)}} = \Varsubarg{\P^{(2)}}{x_{i}^{(2)}} = 1,
\end{align}
which in practice is approximated by
\begin{align}
  \maximize_{c_{1}^{(1)} \in \reals^{p_{1}}, c_{1}^{(2)} \in
    \reals^{p_{2}}} & c_{1}^{(1) T}\hat{\Sigma}_{12}c_{1}^{(2)} \\
  \text{subject to } &c_{1}^{(1) T}\hat{\Sigma}_{11} c_{1}^{(1)} =
  c_{1}^{(2) T}\hat{\Sigma}_{22}c_{1}^{(2)} = 1.
\end{align}

\cite{witten2009penalized} argues for diagonalized CCA, in which the
variance constraints are replaced by unit norm constraints, and
sparsity-inducing $\ell^{1}$ constraints are added,
\begin{align}
  \maximize_{c_{1}^{(1)} \in \reals^{p_{1}}, c_{1}^{(2)} \in
    \reals^{p_{2}}} & c_{1}^{(1) T}\hat{\Sigma}_{12}
  c_{1}^{(2)} \\
  \text{subject to } &\|c_{1}^{(1)}\|_{2}^{2} = \|c_{1}^{(2)}\|_{2}^{2} = 1 \\
  &\|c_{1}^{(1)}\|_{1} \leq \mu_{1} \\
  &\|c_{1}^{(2)}\|_{1} \leq \mu_{2}
\end{align}
which is exactly in the form \ref{eq:pmd_reform} where $X =
\hat{\Sigma}_{12}$, so indeed the PMD framework includes this form of
sparse CCA.

Multiple CCA can also be described in this framework, by replacing
the objective with the sum over all pairwise covariances, $\sum_{l,
  l^{\prime} = 1}^{L} c_{1}^{(l) T}X^{(l)
  T}X^{(l^{\prime})}c_{1}^{(l)^{\prime}}$, and introducing constraints
for each of the $c_{1}^{(l)}$.

\subsubsection{Covariate-assisted Spectral Clustering}

Spectral clustering is an approach to clustering samples using ideas
from graph partitioning. A general question is whether supplmental
data sources can be incorporated into a spectral clustering, while
keeping the similarities defined by some primary dataset. The
community has investigated several possibilities
\cite{binkiewicz2014covariate, jiang2012transfer}; we will describe
the approach in \cite{binkiewicz2014covariate}.

Recall the spectral clustering algorithm. The first step is to build a
weighted graph between all samples, where the edge weights give a
similarity between the connected pair of samples, and call the
associated adjacency matrix $A \in \reals^{n \times n}$. For example,
a common choice is a Gaussian radial basis, $A_{ij} =
\exp{-\frac{1}{2\sigma^{2}}\|x_{i} - x_{j}\|_{2}^{2}}$ for some
variance $\sigma^{2}$; sometimes entries $A_{ij}$ smaller than $\eps$
are thresholded to zero, to take advantage of computational speedups
involved with sparse matrices. Let $D \in \reals^{n \times n}$ be a
diagonal matrix whose elements sum the total edge-weight emanating
from individual nodes, that is, $D_{ii} = \sum_{j = 1}^{n}
A_{ij}$. Take the top $K$ eigenvectors\footnote{$K$ is the parameter
  for the number of clusters to use, it must be chosen ahead of time.}
of $L = D^{-\frac{1}{2}}A D^{-\frac{1}{2}}$, call them $U \in
\reals^{n \times k}$. Then, normalize the rows of $U$ to unit length,
and cluster these rows using $K$-means. These are the final clusters
reported by spectral clustering. Note that, in practice, it is common
to ``regularize'' $L$ by replacing $D$ with $D + \tau I_{n}$, where
$\tau$ is a new tuning parameter.

The essential step is the transformation to the space of eigenvectors
of $L$. A nice interpretation of this transformation is think back to
the weighted graph associated with $A$. Consider a random walk on this
graph, which transitions from $i$ to to some $j \in \{1, \dots, n\}$
with probability proportional to $A_{ij}$; this transition probability
is exactly $L_{ij}$. Suppose the graph associated with $A$ has $K$
separate components, which we write as $V = \cup_{k = 1}^{K}
V\left(K\right)$. Then there are $K$ stationary distributions for this
random walk, with the $k^{th}$ having support confined to
$G\left(k\right)$. In particular, the vectors $u_{k}$ having $i^{th}$
entry $\indic{i \in G\left(k\right)}$ are eigenvectors of $L$, each
with eigenvalue 1. Hence, clustering based on the top $K$ eigenvectors
will assign nodes based on their pattern of $0$'s and $1$'s, which
corresponds to the components $G\left(k\right)$. The idea in spectral
clustering is that, even if the graph is not exactly split across $K$
components, if it is approximately split, then a similar pattern in
the eigenvectors will emerge, and it will still be possible to cluster
them.

Note that spectral clustering is a two-step procedure, the first finds
a low-dimensional representation that can be easily clustered, while
the second performs the actual clustering in this space. To recover
ordinations comparable to those describe elsewhere, it is possible to
stop after the first step, this is sometimes called a ``diffusion
map'' \cite{coifman2005geometric}.

From a high-level, the approach in \cite{binkiewicz2014covariate} is
to mix the transition matrix $L$ obtained from the original data with
a similarity defined independently from the supplementary
data. Specifically, first compute $L$ from similarities based on rows
of $X^{(1)}$. Next, compute the top $K$ eigenvectors $U \in \reals^{n
  \times K}$ of
\begin{align}
  LL^{T} + \alpha X^{(2)}X^{(2) T},
\end{align}
and then row-normalize and cluster as before. Extending the graph
interpretation from before, the matrix $LL^{T}$ can be interpreted as
a two-step transition matrix based on similarities in
$X^{(1)}$. The matrix $X^{(2)}X^{(2) T}$ is an unnormalized empirical
covariance between rows of $X^{(2)}$, and adding it to $LL^{T}$
modifies the transition probabilities between nodes using the
supplemental data. The parameter $\alpha$ trades off the importance of
$X^{(1)}$ vs. $X^{(2)}$ in computing these similarities.

\subsubsection{Matrix Factorization}

\subsubsection{Bayesian Latent Factor Modeling}

\bibliographystyle{unsrt}
\bibliography{microbiome_multitable}

\end{document}
